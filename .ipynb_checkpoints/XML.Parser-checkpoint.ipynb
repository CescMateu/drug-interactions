{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the directory where to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dir = '../LaboCase/Train/'\n",
    "dirs_whereto_parse = [train_dir+'/test_DrugBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the tokens 'START' and 'STOP' at the beginning and end of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entities=[]\n",
    "texts=[]\n",
    "texts_entities = []\n",
    "\n",
    "for directory in dirs_whereto_parse:\n",
    "    name_files=listdir(directory)   # querying all the files that are in that directory\n",
    "    # Parse all these xml files\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in name_files if a.endswith('.xml')]\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            texts_entities = texts_entities + [('START ' + sentence.get('text') + ' STOP',entities)]\n",
    "            entities =[]\n",
    "\n",
    "# texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# print(texts_entities[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIO TAGGER\n",
    "\n",
    "Let's try to tag each sentence with the BIO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "def bio_tagger(text,drugs):\n",
    "    \n",
    "        # Some Preprocessing. I split each word and those ones joined with -\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = sum([word.split('-') for word in tokens if word[0] != '-' and word[-1] != '-'],[])\n",
    "        # print(tokens)\n",
    "        drugs = sum([word.split() for word in drugs],[])\n",
    "        #print(drugs)\n",
    "        \n",
    "        \n",
    "        bio_tagged = []\n",
    "        prev_tag = \"O\"\n",
    "        for token in tokens:\n",
    "            if prev_tag == \"O\" : # Begin NE or continue O\n",
    "                \n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'B'))\n",
    "                    prev_tag = 'B'\n",
    "                else:\n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "                \n",
    "            elif prev_tag == \"B\": # Inside NE\n",
    "                \n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'I'))\n",
    "                    prev_tag = 'I'\n",
    "                else: \n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "                    \n",
    "            elif  prev_tag == \"I\": # Inside NE\n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'I'))\n",
    "                    prev_tag = 'I'\n",
    "                else: \n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "        return bio_tagged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tokens = []\n",
    "tags = []\n",
    "for text,drugs in texts_entities:\n",
    "    tuples = bio_tagger(text,drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "train_set = {'token':tokens,'output':tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STOP'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['token'][len(train_set['token'])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length and capitalization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c1c41691dc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Feature 1: Length of the token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# We have realised that some tokens have length 0 (empty string), so we have decided to remove those rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Feature 1: Length of the token\n",
    "train_df['token_length'] = [len(token) for token in train_set['token']]\n",
    "\n",
    "# We have realised that some tokens have length 0 (empty string), so we have decided to remove those rows\n",
    "train_df = train_df[train_df['token_length'] > 0]\n",
    "\n",
    "# Feature 2: Is the the first letter of the word capitalized?\n",
    "is_capitalized = [row[0].isupper() for row in train_df['token']]\n",
    "train_df['is_capitalized'] = is_capitalized\n",
    "\n",
    "# Feature 3: Is the token completely capitalized?\n",
    "is_total_capitalized = [row.isupper() for row in train_df['token']]\n",
    "train_df['is_total_capitalized'] = is_capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'hello'.isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sufixes and prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# this feature indicates whether the word has a usual prefix/suffix of a drug\n",
    "\n",
    "prefix_feature = []\n",
    "suffix_feature = []\n",
    "\n",
    "prefixes = r'^meth|^eth|^prop|^but|^pent|^hex|^hept|^oct|^non|^dec'\n",
    "suffixes = r'ane$|ene$|yne$|ol$|al$|amine$|cid$|ium$|ether$|ate$|one'\n",
    "\n",
    "for token in train_set['token']:\n",
    "    \n",
    "        if re.search(prefixes,token):\n",
    "            prefix_feature=prefix_feature+[1]\n",
    "        else:\n",
    "            prefix_feature = prefix_feature+[0]\n",
    "            \n",
    "        if re.search(suffixes,token):\n",
    "            suffix_feature=suffix_feature+[1]\n",
    "        else:\n",
    "            suffix_feature = suffix_feature+[0]\n",
    "            \n",
    "train_df['prefix_feature']=prefix_feature\n",
    "train_df['suffix_feature']=suffix_feature\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create a SVM object with the corresponding parameters\n",
    "clf = svm.SVC(gamma=0.001, cache_size = 200, class_weight = None, coef0 = 0.0, \n",
    "              decision_function_shape = None, degree = 3, kernel = 'rbf', \n",
    "              max_iter = -1, probability = False, random_state = None, shrinking = True, \n",
    "              tol = 0.001, C=100.0, verbose = True)\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM\n",
    "X = train_df.loc[:, train_df.columns != target_name]\n",
    "Y = train_df[target_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set.loc[:, train_set.columns != target_name]\n",
    "train_set[target_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set['target_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
