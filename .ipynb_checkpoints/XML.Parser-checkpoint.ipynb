{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from os import listdir\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the directory where to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dir = '../LaboCase/Train/'\n",
    "dirs_whereto_parse = [train_dir+'/DrugBank',train_dir+'/MedLine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the tokens 'START' and 'STOP' at the beginning and end of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('START Milk, milk products, and calcium-rich foods or drugs may impair the absorption of EMCYT. STOP', ['calcium', 'EMCYT'])\n"
     ]
    }
   ],
   "source": [
    "entities=[]\n",
    "texts=[]\n",
    "texts_entities = []\n",
    "\n",
    "for directory in dirs_whereto_parse:\n",
    "    name_files=listdir(directory)   # querying all the files that are in that directory\n",
    "    # parsing all these xml files. I got some problems with a strange .DS_ file in name_files\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in name_files if a.endswith('.xml')]\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            texts_entities = texts_entities + [('START ' + sentence.get('text') + ' STOP',entities)]\n",
    "            entities =[]\n",
    "\n",
    "# texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# print(texts_entities[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIO TAGGER\n",
    "\n",
    "Let's try to tag each sentence with the BIO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "def bio_tagger(text,drugs):\n",
    "    \n",
    "        # Some Preprocessing. I split each word and those ones joined with -\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = sum([word.split('-') for word in tokens],[])\n",
    "        # print(tokens)\n",
    "        drugs = sum([word.split() for word in drugs],[])\n",
    "        #print(drugs)\n",
    "        \n",
    "        \n",
    "        bio_tagged = []\n",
    "        prev_tag = \"O\"\n",
    "        for token in tokens:\n",
    "            if prev_tag == \"O\" : # Begin NE or continue O\n",
    "                \n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'B'))\n",
    "                    prev_tag = 'B'\n",
    "                else:\n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "                \n",
    "            elif prev_tag == \"B\": # Inside NE\n",
    "                \n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'I'))\n",
    "                    prev_tag = 'I'\n",
    "                else: \n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "                    \n",
    "            elif  prev_tag == \"I\": # Inside NE\n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'I'))\n",
    "                    prev_tag = 'I'\n",
    "                else: \n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "        return bio_tagged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tokens = []\n",
    "tags = []\n",
    "for text,drugs in texts_entities:\n",
    "    tuples = bio_tagger(text,drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "train_set = {'tokens':tokens,'labels':tags}\n",
    "df_train = pd.DataFrame(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>Milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>products</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels    tokens\n",
       "0      O     START\n",
       "1      O      Milk\n",
       "2      O         ,\n",
       "3      O      milk\n",
       "4      O  products"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "# Create a SVM object\n",
    "clf = svm.SVC(gamma=0.001, C=100., verbose = True)\n",
    "# Fit our classifier with the data\n",
    "clf.fit(iris.data[:-1], iris.target[:-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
