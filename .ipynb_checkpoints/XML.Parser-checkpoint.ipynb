{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Load needed modules and specify the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "from lxml import etree # XML file parsing\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV # Parameter selection\n",
    "import time # Execution time of some blocks\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import statistics\n",
    "\n",
    "# Import our defined functions\n",
    "from drug_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directories\n",
    "train_dirs_whereto_parse = ['data/small_train_DrugBank']\n",
    "test_dirs_whereto_parse = ['data/small_test_DrugBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train and test data from the XML files\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the token 'STOP' at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('START Formal drug interaction studies have not been conducted with ORENCIA. STOP',\n",
       "  ['ORENCIA']),\n",
       " ('START Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance. STOP',\n",
       "  ['MTX', 'NSAIDs', 'corticosteroids', 'TNF blocking agents', 'abatacept'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRAINING DATA\n",
    "\n",
    "# Initialise the different lists with the data\n",
    "entities=[]\n",
    "texts=[]\n",
    "train_texts_entities = []\n",
    "\n",
    "# Iterate over all the different .xml files located in the specified directories\n",
    "for directory in train_dirs_whereto_parse:\n",
    "    \n",
    "    # Get the names of all the files in the directory and create a 'xml.root' object for\n",
    "    # each xml file\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in listdir(directory) if a.endswith('.xml')]\n",
    "    \n",
    "    # Iterate over all the different 'xml.root' objects to extract the needed information\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            train_texts_entities = train_texts_entities + [('START '+sentence.get('text')+' STOP', entities)]\n",
    "            entities =[]\n",
    "\n",
    "# train_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# Example: \n",
    "# [('I love Ibuprofeno and Frenadol', ['Ibuprofeno', 'Frenadol']), ('Give me a Fluimucil', ['Fluimucil'])]\n",
    "\n",
    "train_texts_entities[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\\n', ['testosterone', 'Plenaxis'])\n"
     ]
    }
   ],
   "source": [
    "## TESTING DATA\n",
    "\n",
    "# Same process as with the training data\n",
    "# In the testing data, for each sentance we have two related files:\n",
    "# - A file with a sentence to be parsed, in which we may encounter drug names (ending with 'text.txt')\n",
    "# - A file with the drug entities recognised in the sentence (ending with 'entities.txt')\n",
    "\n",
    "test_texts = []\n",
    "test_entities = []\n",
    "\n",
    "for directory in test_dirs_whereto_parse:\n",
    "    \n",
    "    # Si no poso el sorted, em llegeix els files amb un ordre aleatori.\n",
    "    # Amb el sorted m'asseguro que els corresponents files text.txt i entities.txt estan en la mateixa posicio\n",
    "    \n",
    "    # Read the pairs of files in alphabetical order\n",
    "    text_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('text.txt')])\n",
    "    entities_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('entities.txt')])\n",
    "    \n",
    "    for file in text_file_names:\n",
    "        file = open(file,'r')\n",
    "        test_texts = test_texts + [file.read()]\n",
    "        \n",
    "    for file in entities_file_names:\n",
    "        read_entities = []\n",
    "        with open(file,'r') as f:\n",
    "            for line in f:\n",
    "                read_entities = read_entities+[' '.join(line.split()[0:-1])] # separo en words, el.limino la ultima i torno a unir\n",
    "                \n",
    "        test_entities.append(read_entities)\n",
    "\n",
    "\n",
    "test_texts_entities=list(zip(test_texts,test_entities))\n",
    "\n",
    "\n",
    "# test_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there.\n",
    "print(test_texts_entities[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO Tagger and Feature Creation\n",
    "\n",
    "In this section we will tag each sentence with the BIO format. For this, we have created a function called 'bioTagger' which will perform the following actions:\n",
    "\n",
    "Given a sentence 'text' and a set of drugs 'drugs', this function returns a list of str that\n",
    "contains a tag for each of the tokens in text. The tags can be either 'B', 'I' or 'O'. 'B' means\n",
    "the token is the first part of a drug entity, 'I' means the token is the continuation of a drug entity,\n",
    "and 'O' means that the token does not belong to a drug entity.\n",
    "\n",
    "Apart from that, we have also downloaded the DrugBank database (ref: https://www.drugbank.ca/) from we will extract all the named entities. We will create a list out of these set of entities and for each token processed, we will check if the token is already in the database, meaning that has a very high probability of being a NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n",
      "/Users/miqueltubaupires/Documents/Data Science/Github/drug-interactions/drug_functions.py:448: UserWarning: This character could not be mapped to any of the options\n",
      "  else: warnings.warn('This character could not be mapped to any of the options')\n"
     ]
    }
   ],
   "source": [
    "# Load the DrugBank list of entities (it has already been processed for the extraction of the NE).\n",
    "# Each line of the file contains a different named entity.\n",
    "with(open('data/DrugBank_names_DB.txt', 'r')) as f:\n",
    "    drugbank_db = f.read().splitlines()\n",
    "        \n",
    "# Initialise the needed lists\n",
    "tokens = []\n",
    "tags = []\n",
    "removed_columns = []\n",
    "features = pd.DataFrame()\n",
    "\n",
    "# Creating StanfordPOStagger. We will need it as a createFeatureVector function parameter\n",
    "jar='Stanford_POStagger/stanford-postagger.jar'\n",
    "model='Stanford_POStagger/models/english-bidirectional-distsim.tagger'\n",
    "st = StanfordPOSTagger(model,jar, encoding='utf-8')\n",
    "\n",
    "# Iterate over all the train entities (tuples of (sentence, drugs)) and apply the bioTagger function\n",
    "for text,drugs in train_texts_entities:\n",
    "    features = pd.concat([features,createFeatureVector(text, drugbank_db,st)])\n",
    "    tuples = bioTagger(text, drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "# computing one-hot coding for 'Aa1-' feature.\n",
    "training_dummies = pd.get_dummies(features['Aa1-'])\n",
    "features = features.drop('Aa1-',axis=1)\n",
    "# joining both data frames\n",
    "for name in training_dummies.columns:\n",
    "    features[name]=training_dummies[name]\n",
    "\n",
    "'''\n",
    "# Remove these features columns that are all 0. We will find many in those indicating pos_tags\n",
    "for column in features.columns.values:\n",
    "    if sum(features[column])==0:\n",
    "        features = features.drop(column,axis=1)\n",
    "        removed_columns.append(column)\n",
    "'''\n",
    "\n",
    "# Create a training set with the features,tokens and the BIO tags\n",
    "train_df = features\n",
    "train_df['token'] = tokens\n",
    "train_df['output'] = tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "204\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_length</th>\n",
       "      <th>prefix_feature</th>\n",
       "      <th>suffix_feature</th>\n",
       "      <th>all_uppercase_letters</th>\n",
       "      <th>all_lowercase_letters</th>\n",
       "      <th>initial_capital_letter</th>\n",
       "      <th>contains_slash</th>\n",
       "      <th>all_letters</th>\n",
       "      <th>all_digits</th>\n",
       "      <th>contains_digit</th>\n",
       "      <th>...</th>\n",
       "      <th>-aA</th>\n",
       "      <th>1</th>\n",
       "      <th>1-</th>\n",
       "      <th>1-A</th>\n",
       "      <th>1-a</th>\n",
       "      <th>A</th>\n",
       "      <th>a</th>\n",
       "      <th>aA</th>\n",
       "      <th>token</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Formal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>drug</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>interaction</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>studies</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_length  prefix_feature  suffix_feature  all_uppercase_letters  \\\n",
       "0             5               0               0                      1   \n",
       "1             6               0               1                      0   \n",
       "2             4               0               0                      0   \n",
       "3            11               0               0                      0   \n",
       "4             7               0               0                      0   \n",
       "\n",
       "   all_lowercase_letters  initial_capital_letter  contains_slash  all_letters  \\\n",
       "0                      0                       1               0            1   \n",
       "1                      0                       1               0            0   \n",
       "2                      1                       0               0            0   \n",
       "3                      1                       0               0            0   \n",
       "4                      1                       0               0            0   \n",
       "\n",
       "   all_digits  contains_digit   ...    -aA  1  1-  1-A  1-a  A  a  aA  \\\n",
       "0           0               0   ...      0  0   0    0    0  1  0   0   \n",
       "1           0               0   ...      0  0   0    0    0  0  0   1   \n",
       "2           0               0   ...      0  0   0    0    0  0  1   0   \n",
       "3           0               0   ...      0  0   0    0    0  0  1   0   \n",
       "4           0               0   ...      0  0   0    0    0  0  1   0   \n",
       "\n",
       "         token  output  \n",
       "0        START       O  \n",
       "1       Formal       O  \n",
       "2         drug       O  \n",
       "3  interaction       O  \n",
       "4      studies       O  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tokens have we tagged with the DrugBank?\n",
    "print(sum(train_df['is_token_in_DrugBank_db'] == 1))\n",
    "\n",
    "# How many tokens are actually tagged with a 'B' or a 'I'?\n",
    "print(sum(train_df['output'] == 'B') + sum(train_df['output'] == 'I'))\n",
    "#train_df[train_df['is_token_in_DrugBank_db'] == 1]\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "X = train_df.loc[:, [all(x) for x in list(zip(train_df.columns!=target_name,train_df.columns!=token_name))]]\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning SVM in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for GridSearchCV:  0.0003750324249267578\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM object with the corresponding tunned parameters\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Look for the best parameters of the SVM model with GridSearchCV\n",
    "start = time.time()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "end = time.time()\n",
    "print('Execution time for GridSearchCV: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time of the SVM:  1.815971851348877\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model with the parameters selected before\n",
    "start = time.time()\n",
    "clf.fit(X,Y)\n",
    "end = time.time()\n",
    "print('Training time of the SVM: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miqueltubaupires/Documents/Data Science/Github/drug-interactions/drug_functions.py:448: UserWarning: This character could not be mapped to any of the options\n",
      "  else: warnings.warn('This character could not be mapped to any of the options')\n",
      "/Users/miqueltubaupires/Documents/Data Science/Github/drug-interactions/drug_functions.py:255: UserWarning: One of the tags was not recognised. Please check the \"bio_tags\" parameter.\n",
      "  warnings.warn('One of the tags was not recognised. Please check the \"bio_tags\" parameter.')\n"
     ]
    }
   ],
   "source": [
    "# Computing training error. If there is a significant drop from training error to test error, we will be suffering \n",
    "# from overfitting\n",
    "\n",
    "train_predictions = []\n",
    "for text,entities in train_texts_entities:\n",
    "    # print('text: ', text)\n",
    "    # print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # computing predictions\n",
    "    features = createFeatureVector(text, drugbank_db,st)\n",
    "    \n",
    "    # computing one-hot coding for 'Aa1-' feature.\n",
    "    dummies = pd.get_dummies(features['Aa1-'])\n",
    "    features = features.drop('Aa1-',axis=1)\n",
    "    # joining both data frames\n",
    "    for name in dummies.columns:\n",
    "        features[name]=dummies[name]\n",
    "    \n",
    "    # adding those columns related to Aa1- that we cannot see with the sentence in question\n",
    "    for name in training_dummies.columns:\n",
    "        if name not in dummies.columns:\n",
    "            features[name]=[0]*len(dummies[dummies.columns.values[0]])\n",
    "    \n",
    "  \n",
    "    predicted_tags = clf.predict(features)\n",
    "    \n",
    "    \n",
    "    train_predictions.append((list(predicted_tags),entities,text)) \n",
    "    # print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    pred_entities = bioTagsToEntities(tokens = tokens, bio_tags = predicted_tags)\n",
    "    # print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "#print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train precision:  62.32258064516129\n",
      "train recall:  69.06451612903226\n",
      "F1 train:  65.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miqueltubaupires/Documents/Data Science/Github/drug-interactions/drug_functions.py:255: UserWarning: One of the tags was not recognised. Please check the \"bio_tags\" parameter.\n",
      "  warnings.warn('One of the tags was not recognised. Please check the \"bio_tags\" parameter.')\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "train_precision = []\n",
    "train_recall = []\n",
    "for tags, true_entities, text in train_predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = bioTagsToEntities(tokens,tags)\n",
    "    train_precision = train_precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    train_recall = train_recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(train_precision)\n",
    "avg_recall = statistics.mean(train_recall)\n",
    "print('train precision: ',avg_precision)\n",
    "print('train recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1_train = round((2*avg_precision*avg_recall) / (avg_precision + avg_recall),2)\n",
    "print('F1 train: ', F1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  No drug, nutritional supplement, food or herb interactions have yet been reported.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  [] \n",
      "\n",
      "text:  No formal drug/drug interaction studies with Plenaxis were performed.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'B' 'O' 'O' 'O' 'B' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['drug/drug', 'Plenaxis'] \n",
      "\n",
      "text:  Cytochrome P-450 is not known to be involved in the metabolism of Plenaxis.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['metabolism'] \n",
      "\n",
      "text:  Plenaxis is highly bound to plasma proteins (96 to 99%).\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['proteins'] \n",
      "\n",
      "text:  Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\n",
      "\n",
      "real entities:  ['testosterone', 'Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis', 'measuring', 'testosterone'] \n",
      "\n",
      "text:  Serum transaminase levels should be obtained before starting treatment with Plenaxis and periodically during treatment.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis'] \n",
      "\n",
      "text:  Periodic measurement of serum PSA levels may also be considered.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  [] \n",
      "\n",
      "text:  Formal drug interaction studies have not been conducted with ORENCIA.\n",
      "\n",
      "real entities:  ['ORENCIA'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['Formal'] \n",
      "\n",
      "text:  Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance.\n",
      "\n",
      "real entities:  ['abatacept', 'corticosteroids', 'TNF blocking agents', 'MTX', 'NSAIDs'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'B' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['MTX', 'NSAIDs', 'corticosteroids', 'abatacept'] \n",
      "\n",
      "text:  The majority of patients in RA clinical studies received one or more of the following concomitant medications with ORENCIA: MTX, NSAIDs, corticosteroids, TNF blocking agents, azathioprine, chloroquine, gold, hydroxychloroquine, leflunomide, sulfasalazine, and anakinra.\n",
      "\n",
      "real entities:  ['anakinra', 'sulfasalazine', 'corticosteroids', 'NSAIDs', 'MTX', 'ORENCIA', 'gold', 'chloroquine', 'azathioprine', 'TNF blocking agents', 'leflunomide', 'hydroxychloroquine'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O'\n",
      " 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O'\n",
      " 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['medications', 'ORENCIA', 'MTX', 'NSAIDs', 'corticosteroids', 'azathioprine', 'chloroquine', 'hydroxychloroquine', 'leflunomide', 'sulfasalazine'] \n",
      "\n",
      "text:  Concurrent administration of a TNF antagonist with ORENCIA has been associated with an increased risk of serious infections and no significant additional efficacy over use of the TNF antagonists alone.\n",
      "\n",
      "real entities:  ['TNF antagonist', 'ORENCIA', 'TNF antagonists'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miqueltubaupires/Documents/Data Science/Github/drug-interactions/drug_functions.py:448: UserWarning: This character could not be mapped to any of the options\n",
      "  else: warnings.warn('This character could not be mapped to any of the options')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA', 'significant', 'efficacy'] \n",
      "\n",
      "text:  Concurrent therapy with ORENCIA and TNF antagonists is not recommended.\n",
      "\n",
      "real entities:  ['TNF antagonists', 'ORENCIA'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA'] \n",
      "\n",
      "text:  There is insufficient experience to assess the safety and efficacy of ORENCIA administered concurrently with anakinra, and therefore such use is not recommended.\n",
      "\n",
      "real entities:  ['ORENCIA', 'anakinra'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'B' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA', 'anakinra'] \n",
      "\n",
      "text:  Formal drug interaction studies with Abciximab have not been conducted.\n",
      "\n",
      "real entities:  ['Abciximab'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Formal', 'Abciximab'] \n",
      "\n",
      "text:  Abciximab has been administered to patients with ischemic heart disease treated concomitantly with a broad range of medications used in the treatment of angina myocardial infarction and hypertension.\n",
      "\n",
      "real entities:  ['Abciximab'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Abciximab', 'ischemic', 'disease', 'medications'] \n",
      "\n",
      "text:  These medications have included heparin, warfarin, beta-adrenergic receptor blockers, calcium channel antagonists, angiotensin converting enzyme inhibitors, intravenous and oral nitrates, ticlopidine, and aspirin.\n",
      "\n",
      "real entities:  ['aspirin', 'ticlopidine', 'nitrates', 'angiotensin converting enzyme inhibitors', 'calcium channel antagonists', 'beta-adrenergic receptor blockers', 'warfarin', 'heparin'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'B' 'B' 'O' 'O' 'B' 'O' 'O' 'O' 'B' 'O'\n",
      " 'O' 'B' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['heparin', 'warfarin', 'beta-adrenergic', 'receptor', 'calcium', 'angiotensin', 'inhibitors', 'nitrates', 'ticlopidine'] \n",
      "\n",
      "text:  Heparin, other anticoagulants, thrombolytics, and anti platelet agents are associated with an increase in bleeding.\n",
      "\n",
      "real entities:  ['Heparin', 'anticoagulants', 'thrombolytics', 'anti platelet agents'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O'] \n",
      "\n",
      "predicted entities:  ['Heparin', 'anticoagulants', 'platelet'] \n",
      "\n",
      "text:  Patients with HACA titers may have allergic or hypersensitivity reactions when treated with other diagnostic or therapeutic monoclonal antibodies.\n",
      "\n",
      "real entities:  ['therapeutic monoclonal antibodies', 'diagnostic monoclonal antibodies'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B'\n",
      " 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['monoclonal'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text,entities in test_texts_entities:\n",
    "    print('text: ', text)\n",
    "    print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # computing predictions\n",
    "    features = createFeatureVector(text, drugbank_db,st)\n",
    "    \n",
    "    # computing one-hot coding for 'Aa1-' feature.\n",
    "    dummies = pd.get_dummies(features['Aa1-'])\n",
    "    features = features.drop('Aa1-',axis=1)\n",
    "    # joining both data frames\n",
    "    for name in dummies.columns:\n",
    "        features[name]=dummies[name]\n",
    "    \n",
    "    # adding those columns related to Aa1- that we cannot see with the sentence in question\n",
    "    for name in training_dummies.columns:\n",
    "        if name not in dummies.columns:\n",
    "            features[name]=[0]*len(dummies[dummies.columns.values[0]])\n",
    "            \n",
    "    '''\n",
    "    # removing those columns deleted when training the classifier\n",
    "    for column in features.columns.values:\n",
    "        if column in removed_columns:\n",
    "            features = features.drop(column,axis=1)\n",
    "    '''\n",
    "    \n",
    "    predicted_tags = clf.predict(features)\n",
    "    \n",
    "    predictions.append((list(predicted_tags),entities,text)) \n",
    "    print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    pred_entities = bioTagsToEntities(tokens = tokens, bio_tags = predicted_tags)\n",
    "    print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "#print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define a function that recover's the whole drug name from BIO taggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be based on $$F1=\\frac{2*precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover all the words from the predicted bio_tags and try to compute F1 for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  45.888888888888886\n",
      "recall:  52.111111111111114\n",
      "F1:  48.8\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "recall = []\n",
    "for tags, true_entities, text in predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = bioTagsToEntities(tokens,tags)\n",
    "    precision = precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    recall = recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "print('precision: ',avg_precision)\n",
    "print('recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1 = round((2*avg_precision*avg_recall) / (avg_precision + avg_recall),2)\n",
    "print('F1: ', F1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
