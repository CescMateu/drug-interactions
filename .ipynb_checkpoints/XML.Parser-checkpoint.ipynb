{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the directory where to parse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories for  CESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_dir = '../LaboCase/Train/'\\ndirs_whereto_parse = [train_dir+'/test_DrugBank']\\n\""
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_dir = '../LaboCase/Train/'\n",
    "dirs_whereto_parse = [train_dir+'/test_DrugBank']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories for Miki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_dir = '/Users/miqueltubaupires/Documents/Master/3r QUATRIMESTRE/AHLT/Lab/ddi/Train'\n",
    "train_dirs_whereto_parse = [train_dir+'/DrugBank',train_dir+'/MedLine']\n",
    "test_dir = '/Users/miqueltubaupires/Documents/Master/3r QUATRIMESTRE/AHLT/Lab/ddi/Test'\n",
    "'''\n",
    "train_dir = '/Users/miqueltubaupires/Documents/Master/3r QUATRIMESTRE/AHLT/Lab/ddi/'\n",
    "train_dirs_whereto_parse = [train_dir+'/Small Train']\n",
    "\n",
    "test_dir = '/Users/miqueltubaupires/Documents/Master/3r QUATRIMESTRE/AHLT/Lab/ddi/'\n",
    "test_dirs_whereto_parse = [test_dir+'/Small Test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading train data\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the tokens 'START' and 'STOP' at the beginning and end of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities=[]\n",
    "texts=[]\n",
    "train_texts_entities = []\n",
    "\n",
    "for directory in train_dirs_whereto_parse:\n",
    "    name_files=listdir(directory)   # querying all the files that are in that directory\n",
    "    # Parse all these xml files\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in name_files if a.endswith('.xml')]\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            train_texts_entities = train_texts_entities + [('START ' + sentence.get('text') + ' STOP',entities)]\n",
    "            entities =[]\n",
    "\n",
    "# train_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# print(texts_entities[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "test_entities = []\n",
    "for directory in test_dirs_whereto_parse:\n",
    "    name_files = listdir(directory)\n",
    "    # Si no poso el sorted, em llegeix els files amb un ordre aleatori.\n",
    "    # Amb el sorted m'asseguro que els corresponents files text.txt i entities.txt estan en la mateixa posicio\n",
    "    text_file_names = sorted([directory+'/'+a for a in name_files if a.endswith('text.txt')])\n",
    "    entities_file_names = sorted([directory+'/'+a for a in name_files if a.endswith('entities.txt')])\n",
    "    for file in text_file_names:\n",
    "        file = open(file,'r')\n",
    "        test_texts = test_texts + [file.read()]\n",
    "    for file in entities_file_names:\n",
    "        read_entities = []\n",
    "        with open(file,'r') as f:\n",
    "            for line in f:\n",
    "                read_entities = read_entities+[' '.join(line.split()[0:-1])] # separo en words, el.limino la ultima i torno a unir\n",
    "        test_entities.append(read_entities)\n",
    "        \n",
    "# test_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "test_texts_entities=list(zip(test_texts,test_entities))\n",
    "# print(test_texts_entities[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIO TAGGER\n",
    "\n",
    "Let's try to tag each sentence with the BIO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "def bio_tagger(text,drugs):\n",
    "    \n",
    "        # Some Preprocessing. I split each word and those ones joined with -\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = sum([word.split('-') for word in tokens if word[0] != '-' and word[-1] != '-'],[])\n",
    "        # print(tokens)\n",
    "        drugs = sum([word.split() for word in drugs],[])\n",
    "        #print(drugs)\n",
    "        \n",
    "        \n",
    "        bio_tagged = []\n",
    "        prev_tag = \"O\"\n",
    "        for token in tokens:\n",
    "            if prev_tag == \"O\" : # Begin NE or continue O\n",
    "                \n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'B'))\n",
    "                    prev_tag = 'B'\n",
    "                else:\n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "                \n",
    "            elif prev_tag == \"B\": # Inside NE\n",
    "                \n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'I'))\n",
    "                    prev_tag = 'I'\n",
    "                else: \n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "                    \n",
    "            elif  prev_tag == \"I\": # Inside NE\n",
    "                if token in drugs:\n",
    "                    bio_tagged.append((token,'I'))\n",
    "                    prev_tag = 'I'\n",
    "                else: \n",
    "                    bio_tagged.append((token,'O'))\n",
    "                    prev_tag = 'O'\n",
    "        return bio_tagged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "tags = []\n",
    "for text,drugs in train_texts_entities:\n",
    "    tuples = bio_tagger(text,drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "train_set = {'token':tokens,'output':tags}\n",
    "train_df = pd.DataFrame(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>formal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>assessments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  output        token\n",
       "0      O        START\n",
       "1      O           No\n",
       "2      O       formal\n",
       "3      O  assessments\n",
       "4      O           of"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_capitalized</th>\n",
       "      <th>is_total_capitalized</th>\n",
       "      <th>output</th>\n",
       "      <th>prefix_feature</th>\n",
       "      <th>suffix_feature</th>\n",
       "      <th>token</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>formal</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>assessments</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_capitalized  is_total_capitalized output  prefix_feature  \\\n",
       "0               1                     1      O               0   \n",
       "1               1                     0      O               0   \n",
       "2               0                     0      O               0   \n",
       "3               0                     0      O               0   \n",
       "4               0                     0      O               0   \n",
       "\n",
       "   suffix_feature        token  token_length  \n",
       "0               0        START             5  \n",
       "1               0           No             2  \n",
       "2               1       formal             6  \n",
       "3               0  assessments            11  \n",
       "4               0           of             2  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_vector(tokenized_sentence):\n",
    "    feature_vector = {}\n",
    "    # Feature 1: Length of the token\n",
    "    feature_vector['token_length'] = [len(token) for token in tokenized_sentence]\n",
    "\n",
    "    # Feature 2: Is the the first letter of the word capitalized?\n",
    "    is_capitalized = [1 if row[0].isupper() else 0 for row in tokenized_sentence]\n",
    "    feature_vector['is_capitalized'] = is_capitalized\n",
    "\n",
    "    # Feature 3: Is the token completely capitalized?\n",
    "    is_total_capitalized = [1 if row.isupper() else 0 for row in tokenized_sentence]\n",
    "    feature_vector['is_total_capitalized'] = is_total_capitalized\n",
    "    \n",
    "    # Feature 4 & 5: Prefixes and Suffixes\n",
    "\n",
    "    prefix_feature = []\n",
    "    suffix_feature = []\n",
    "\n",
    "    prefixes = r'^meth|^eth|^prop|^but|^pent|^hex|^hept|^oct|^non|^dec'\n",
    "    suffixes = r'ane$|ene$|yne$|ol$|al$|amine$|cid$|ium$|ether$|ate$|one$'\n",
    "\n",
    "    for token in tokenized_sentence:\n",
    "\n",
    "            if re.search(prefixes,token):\n",
    "                prefix_feature=prefix_feature+[1]\n",
    "            else:\n",
    "                prefix_feature = prefix_feature+[0]\n",
    "\n",
    "            if re.search(suffixes,token):\n",
    "                suffix_feature=suffix_feature+[1]\n",
    "            else:\n",
    "                suffix_feature = suffix_feature+[0]\n",
    "\n",
    "    feature_vector['prefix_feature']=prefix_feature\n",
    "    feature_vector['suffix_feature']=suffix_feature\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# feature vector\n",
    "features = feature_vector(train_set['token'])\n",
    "\n",
    "# joining two dictionaries\n",
    "train_set = {**train_set,**features}\n",
    "# creating the data frame\n",
    "train_df = pd.DataFrame(train_set)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create a SVM object with the corresponding parameters\n",
    "clf = svm.SVC(gamma=0.001, cache_size = 200, class_weight = None, coef0 = 0.0, \n",
    "              decision_function_shape = None, degree = 3, kernel = 'rbf', \n",
    "              max_iter = -1, probability = False, random_state = None, shrinking = True, \n",
    "              tol = 0.001, C=100.0, verbose = True)\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "X = train_df.loc[:, [all(x) for x in list(zip(train_df.columns!=target_name,train_df.columns!=token_name))]]\n",
    "# X = train_df.loc[:,train_df.columns!=target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_capitalized</th>\n",
       "      <th>is_total_capitalized</th>\n",
       "      <th>prefix_feature</th>\n",
       "      <th>suffix_feature</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_capitalized  is_total_capitalized  prefix_feature  suffix_feature  \\\n",
       "0               1                     1               0               0   \n",
       "1               1                     0               0               0   \n",
       "2               0                     0               0               1   \n",
       "3               0                     0               0               0   \n",
       "4               0                     0               0               0   \n",
       "\n",
       "   token_length  \n",
       "0             5  \n",
       "1             2  \n",
       "2             6  \n",
       "3            11  \n",
       "4             2  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=pd.get_dummies(train_df[target_name])\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          START\n",
       "1             No\n",
       "2         formal\n",
       "3    assessments\n",
       "4             of\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[token_name].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with just one test text. Le'ts tokenize it, and create its feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'have', 'been', 'no', 'studies', 'of', 'the', 'interaction', 'of', 'methyl', 'aminolevulinate', 'cream', 'with', 'any', 'other', 'drugs', ',', 'including', 'local', 'anesthetics', '.']\n"
     ]
    }
   ],
   "source": [
    "token_test_text = nltk.word_tokenize(test_texts[6])\n",
    "print(token_test_text)\n",
    "\n",
    "features = pd.DataFrame(feature_vector(token_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_capitalized</th>\n",
       "      <th>is_total_capitalized</th>\n",
       "      <th>prefix_feature</th>\n",
       "      <th>suffix_feature</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_capitalized  is_total_capitalized  prefix_feature  suffix_feature  \\\n",
       "0               1                     0               0               0   \n",
       "1               0                     0               0               0   \n",
       "2               0                     0               0               0   \n",
       "3               0                     0               0               0   \n",
       "4               0                     0               0               0   \n",
       "\n",
       "   token_length  \n",
       "0             5  \n",
       "1             4  \n",
       "2             4  \n",
       "3             2  \n",
       "4             7  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  If you are taking these medicines together or you have further questions about drug interactions talk to your doctor or pharmacist. \n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O'] \n",
      "\n",
      "text:  Consult your doctor or pharmacist if you are taking any of the following: seizure medications antibiotics warfarin medications to help you sleep\n",
      "\n",
      "real entities:  ['warfarin', 'antibiotics'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "text:  Infergen should be used cautiously in patients who are receiving agents that are known to cause myelosuppression or with agents known to be metabolized via the cytochrome P-450 pathway.9 Patients taking drugs that are metabolized by this pathway should be monitored closely for changes in the therapeutic and/or toxic levels of concomitant drugs.\n",
      "\n",
      "real entities:  ['Infergen'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "text:  Although no drug/drug interactions were discovered during the clinical trials of MARINOL Capsules, cannabinoids may interact with other medications through both metabolic and pharmacodynamic mechanisms. \n",
      "\n",
      "real entities:  ['MARINOL', 'cannabinoids'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "text:  Dronabinol is highly protein bound to plasma proteins, and therefore, might displace other protein-bound drugs. \n",
      "\n",
      "real entities:  ['Dronabinol'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "text:  No specific interactions are known at this time.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "text:  There have been no studies of the interaction of methyl aminolevulinate cream with any other drugs, including local anesthetics. \n",
      "\n",
      "real entities:  ['methyl aminolevulinate', 'anesthetics'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O'] \n",
      "\n",
      "text:  It is possible that concomitant use of other known photosensitizing agents might increase the photosensitivity reaction of actinic keratoses treated with methyl aminolevulinate cream.\n",
      "\n",
      "real entities:  ['methyl aminolevulinate'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'B' 'O' 'O'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text,entities in test_texts_entities:\n",
    "    # print('text: ', text)\n",
    "    # print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_tags = clf.predict(pd.DataFrame(feature_vector(tokens)))\n",
    "    predictions.append((list(predicted_tags),entities,text)) \n",
    "    # print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "# print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, le'ts define a function that recover's the whole drug name from BIO taggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_tags_to_entities(tokens,bio_tags):\n",
    "    entities = []\n",
    "    prev_tag = 'O'\n",
    "    word = ''\n",
    "    for idx in range(0,len(bio_tags)-1):\n",
    "        tag = bio_tags[idx]\n",
    "        if tag=='B':\n",
    "            if prev_tag in ['B','I']:\n",
    "                # si trobo una nova B i la previa era B o I, envio la word previa\n",
    "                entities = entities + [word]\n",
    "            word = tokens[idx]\n",
    "            prev_tag='B'\n",
    "        elif tag =='I':\n",
    "            # si trobo una I, actualitzo la word\n",
    "            word == word + tokens[idx]\n",
    "            prev_tag='I'\n",
    "        elif tag == 'O' and prev_tag in ['B','I']:\n",
    "            # si em trobo una O pero abans tenia una B o una I, envio la word previa\n",
    "            entities = entities + [word]\n",
    "            prev_tag='O'\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # print(tokens)\n",
    "    # print(entities)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "\n",
    "# Un exemple aprofitant l'exemple de prediccio d'abans\n",
    "# bio_tags_to_entities(token_test_text,clf.predict(pd.DataFrame(feature_vector(token_test_text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be based on $$F1=\\frac{2*precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest exemple m'ha ajudat a entendre com calcular la precision i la recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "true = ['hola','que','ca','bo']\n",
    "pred = ['hola','que','pet']\n",
    "\n",
    "print(round(len([word for word in pred if word in true])/len(pred),2))\n",
    "print(round(len([word for word in pred if word in true])/len(true),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(pred_ent),2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(true_ent),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover all the words from the predicted bio_tags and try to compute F1 for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "precision = []\n",
    "recall = []\n",
    "for tags, true_entities, text in predictions:\n",
    "    # I need the tokens for the bio_tags_to_entities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = bio_tags_to_entities(tokens,tags)\n",
    "    precision = precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    recall = recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "print(avg_precision)\n",
    "print(avg_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
