{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - MIRI\n",
    "# Drugs Interaction Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from IPython.display import display # For displaying DataFrames correctly in Jupyter\n",
    "from sklearn import svm\n",
    "import scipy.stats # for RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, train_test_split # Parameter selection\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "\n",
    "# Other libraries\n",
    "import time # Execution time of some blocks\n",
    "import statistics\n",
    "\n",
    "# Import our own defined functions\n",
    "from xlm_parsers_functions import *\n",
    "from drug_interaction_functions import *\n",
    "from drug_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/small_train_DrugBank/'\n",
    "filename = 'Acamprosate_ddi.xml'\n",
    "tree = ET.parse(data_dir + filename)\n",
    "# Create a list of lists with the interactions of the file\n",
    "train_text_entities_relations = listDDIFromXML(tree.getroot())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of this part\n",
    "In this second part of the project, we will focus on two different things: \n",
    "1. Detection of interactions between drugs\n",
    "2. Classification of each drug-drug interaction according to one of the following types:\n",
    "    - Advice: 'Interactions may be expected, and Uroxatral should not be used in combination with other alpha-blockers.'\n",
    "    - Effect: 'In uninfected volunteers, 46% developed rash while receiving Sustiva and Clarithromycin.'\n",
    "    - Mechanism: 'Grepafloxacin is a competitive inhibitor of the metabolism of theophylline'.\n",
    "    - Int: The interaction of omeprazole and ketoconazole has been stablished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the XML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DrugBank and MedLine files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the data paths\n",
    "train_data_dir = 'data/small_train_DrugBank/'\n",
    "test_data_dir = 'data/small_test_DrugBank/'\n",
    "\n",
    "# Read the data from the specified directories\n",
    "DrugBank_df = readTrainingData(train_data_dir)\n",
    "\n",
    "# Select the initial columns from which we will compute the features for each row\n",
    "train_df = DrugBank_df[['sentence_text', 'e1_name', 'e2_name', 'list_entities', 'interaction', 'interaction_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of features\n",
    "Before training our model, we need to come up with features to help us determine whether there is a relationship between the two drugs or not.\n",
    "\n",
    "Some ideas for features are the following:\n",
    "- Does the sentence contain a modal verb (should, must,...) between the two entities?\n",
    "- Word bigrams: This is a binary feature for all word bigrams that appeared more than once in the corpus, indicating the presence or absence of each such bigram in the sentence\n",
    "- Number of words between a pair of drugs\n",
    "- Number of drugs between a pair of drugs\n",
    "- POS of words between a pair of drugs: This is a binary feature for word POS tags obtained from POS tagging, and indicates the presence or absence of each POS between the two main drugs.\n",
    "- Path between a pair of drugs: Path between two main drugs in the parse tree is another feature in our system. Because syntactic paths are in general a sparse feature, we reduced the sparsity by collapsing identical adjacent non-terminal labels. E.g., NP-S-VP-VP-NP is converted to NP-S-VP-NP. This technique decreased the number of paths by 24.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset() List of all POS tags from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_df = createMorphologicFeatures(train_df)\n",
    "end = time.time()\n",
    "print('Time for creating morphological features: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_df = createOrtographicFeatures(train_df)\n",
    "end = time.time()\n",
    "print('Time for creating ortographic features: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_df = createContextFeatures(train_df)\n",
    "end = time.time()\n",
    "print('Time for creating context features: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 5\n",
    "display(train_df.head())\n",
    "#train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables preprocessing\n",
    "As we are working with the sklearn.SVM machine learning model, in this case we need our output variable ('interaction') to be a binary variable encoded with 0 and 1's. For this purpose, we will use the pd.replace function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_encoding = {'interaction': {'true':1, 'false':0}}\n",
    "train_df.replace(new_encoding, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the classifier - SVM\n",
    "\n",
    "### Creation of the training, validation and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'interaction'\n",
    "sentence_name = 'sentence_text'\n",
    "list_entities_name = 'list_entities'\n",
    "ent_1_name = 'e1_name'\n",
    "ent_2_name = 'e2_name'\n",
    "var_not_incl = ['sentence_text', 'e1_name', 'e2_name']\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "# In this first step we will create a whole dataset with 100% of the data, which we will\n",
    "# split in the following steps into training, validation and testing data\n",
    "X = train_df.loc[:, [all(x) for x in list(zip( \n",
    "                train_df.columns != target_name,\n",
    "                train_df.columns != list_entities_name))]]\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our dataset with 100% of the data created, we will create the training, validation and testing datasets. For this part of the project we have decided to split the dataset with the following proportions (60, 20, 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 16273\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed, shuffle = True)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=seed, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "display(X.head())\n",
    "display(Y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a SVM object with the corresponding tunned parameters\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Look for the best parameters of the SVM model with GridSearchCV\n",
    "start = time.time()\n",
    "clf = RandomizedSearchCV(svc,{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n",
    "                              'kernel': ['rbf'], 'class_weight':['balanced', None]},n_iter=40,n_jobs=-1)\n",
    "clf.fit(X_val.drop(var_not_incl, axis = 1), Y_val)\n",
    "end = time.time()\n",
    "print('Validating time of the SVM: ', str(end - start),'\\n')\n",
    "\n",
    "print('Best estimator: ', clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the SVM model with the parameters selected before\n",
    "start = time.time()\n",
    "model = clf.best_estimator_\n",
    "model.fit(X_train.drop(var_not_incl, axis=1), Y_train)\n",
    "end = time.time()\n",
    "print('Training time of the SVM: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test.drop(var_not_incl, axis = 1))\n",
    "true = np.array(Y_test)\n",
    "print(pred, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Precision: ', round(computePrecision(true=true, pred=pred)*100, 1))\n",
    "print('Recall: ', round(computeRecall(true=true, pred=pred)*100, 1))\n",
    "print('F1: ', round(computeF1(true = true, pred = pred)*100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test['real_interaction'] = true\n",
    "X_test['pred_interaction'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test[X_test['pred_interaction'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "['ent1_contains_numbers',\n",
    " 'ent1_has_uppercase',\n",
    " 'ent1_all_uppercase',\n",
    " 'ent1_initial_capital',\n",
    " 'ent1_contains_slash',\n",
    " 'ent1_contains_dash',\n",
    " 'ent1_n_tokens',\n",
    " 'ent1_contains_punctuation',\n",
    " 'ent1_init_digit',\n",
    " 'ent1_single_digit',\n",
    " 'ent1_contains_roman',\n",
    " 'ent1_end_punctuation',\n",
    " 'ent1_caps_mix',\n",
    " 'ent2_contains_numbers',\n",
    " 'ent2_has_uppercase',\n",
    " 'ent2_all_uppercase',\n",
    " 'ent2_initial_capital',\n",
    " 'ent2_contains_slash',\n",
    " 'ent2_contains_dash',\n",
    " 'ent2_n_tokens',\n",
    " 'ent2_contains_punctuation',\n",
    " 'ent2_init_digit',\n",
    " 'ent2_single_digit',\n",
    " 'ent2_contains_roman',\n",
    " 'ent2_end_punctuation',\n",
    " 'ent2_caps_mix',\n",
    " 'n_modal_verbs_bw_entities',\n",
    " 'n_tokens_bw_entities',\n",
    " 'n_entities_bw_entities']\n",
    " \n",
    "Precision:  62.8\n",
    "Recall:  19.3\n",
    "F1:  29.6\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "['ent1_contains_prefix_alk',\n",
    " 'ent1_contains_prefix_meth',\n",
    " 'ent1_contains_prefix_eth',\n",
    " 'ent1_contains_prefix_prop',\n",
    " 'ent1_contains_prefix_but',\n",
    " 'ent1_contains_prefix_pent',\n",
    " 'ent1_contains_prefix_hex',\n",
    " 'ent1_contains_prefix_hept',\n",
    " 'ent1_contains_prefix_oct',\n",
    " 'ent1_contains_prefix_non',\n",
    " 'ent1_contains_prefix_dec',\n",
    " 'ent1_contains_prefix_undec',\n",
    " 'ent1_contains_prefix_dodec',\n",
    " 'ent1_contains_prefix_eifcos',\n",
    " 'ent1_contains_prefix_di',\n",
    " 'ent1_contains_prefix_tri',\n",
    " 'ent1_contains_prefix_tetra',\n",
    " 'ent1_contains_prefix_penta',\n",
    " 'ent1_contains_prefix_hexa',\n",
    " 'ent1_contains_prefix_hepta',\n",
    " 'ent1_contains_suffix_ane',\n",
    " 'ent1_contains_suffix_ene',\n",
    " 'ent1_contains_suffix_yne',\n",
    " 'ent1_contains_suffix_yl',\n",
    " 'ent1_contains_suffix_ol',\n",
    " 'ent1_contains_suffix_al',\n",
    " 'ent1_contains_suffix_oic',\n",
    " 'ent1_contains_suffix_one',\n",
    " 'ent1_contains_suffix_ate',\n",
    " 'ent1_contains_suffix_amine',\n",
    " 'ent1_contains_suffix_amide',\n",
    " 'ent2_contains_prefix_alk',\n",
    " 'ent2_contains_prefix_meth',\n",
    " 'ent2_contains_prefix_eth',\n",
    " 'ent2_contains_prefix_prop',\n",
    " 'ent2_contains_prefix_but',\n",
    " 'ent2_contains_prefix_pent',\n",
    " 'ent2_contains_prefix_hex',\n",
    " 'ent2_contains_prefix_hept',\n",
    " 'ent2_contains_prefix_oct',\n",
    " 'ent2_contains_prefix_non',\n",
    " 'ent2_contains_prefix_dec',\n",
    " 'ent2_contains_prefix_undec',\n",
    " 'ent2_contains_prefix_dodec',\n",
    " 'ent2_contains_prefix_eifcos',\n",
    " 'ent2_contains_prefix_di',\n",
    " 'ent2_contains_prefix_tri',\n",
    " 'ent2_contains_prefix_tetra',\n",
    " 'ent2_contains_prefix_penta',\n",
    " 'ent2_contains_prefix_hexa',\n",
    " 'ent2_contains_prefix_hepta',\n",
    " 'ent2_contains_suffix_ane',\n",
    " 'ent2_contains_suffix_ene',\n",
    " 'ent2_contains_suffix_yne',\n",
    " 'ent2_contains_suffix_yl',\n",
    " 'ent2_contains_suffix_ol',\n",
    " 'ent2_contains_suffix_al',\n",
    " 'ent2_contains_suffix_oic',\n",
    " 'ent2_contains_suffix_one',\n",
    " 'ent2_contains_suffix_ate',\n",
    " 'ent2_contains_suffix_amine',\n",
    " 'ent2_contains_suffix_amide',\n",
    " 'ent1_contains_numbers',\n",
    " 'ent1_has_uppercase',\n",
    " 'ent1_all_uppercase',\n",
    " 'ent1_initial_capital',\n",
    " 'ent1_contains_slash',\n",
    " 'ent1_contains_dash',\n",
    " 'ent1_n_tokens',\n",
    " 'ent1_contains_punctuation',\n",
    " 'ent1_init_digit',\n",
    " 'ent1_single_digit',\n",
    " 'ent1_contains_roman',\n",
    " 'ent1_end_punctuation',\n",
    " 'ent1_caps_mix',\n",
    " 'ent2_contains_numbers',\n",
    " 'ent2_has_uppercase',\n",
    " 'ent2_all_uppercase',\n",
    " 'ent2_initial_capital',\n",
    " 'ent2_contains_slash',\n",
    " 'ent2_contains_dash',\n",
    " 'ent2_n_tokens',\n",
    " 'ent2_contains_punctuation',\n",
    " 'ent2_init_digit',\n",
    " 'ent2_single_digit',\n",
    " 'ent2_contains_roman',\n",
    " 'ent2_end_punctuation',\n",
    " 'ent2_caps_mix',\n",
    " 'n_modal_verbs_bw_entities',\n",
    " 'n_tokens_bw_entities',\n",
    " 'n_entities_bw_entities']\n",
    "\n",
    "Precision:  62.2\n",
    "Recall:  21.4\n",
    "F1:  31.8\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(X_train.drop(var_not_incl, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
