{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Load needed modules and specify the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "from lxml import etree # XML file parsing\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV # Parameter selection\n",
    "import time # Execution time of some blocks\n",
    "\n",
    "# Import our defined functions\n",
    "from drug_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directories\n",
    "# Maik, he afegit les dades dins el mateix repositori de forma que tinguem els mateixos paths i \n",
    "# no haguem de canviarlos cada cop que entrem a aquest file.\n",
    "\n",
    "train_dirs_whereto_parse = ['data/small_train_DrugBank']\n",
    "test_dirs_whereto_parse = ['data/small_test_DrugBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train and test data from the XML files\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the token 'STOP' at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('START Formal drug interaction studies have not been conducted with ORENCIA. STOP',\n",
       "  ['ORENCIA']),\n",
       " ('START Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance. STOP',\n",
       "  ['MTX', 'NSAIDs', 'corticosteroids', 'TNF blocking agents', 'abatacept'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRAINING DATA\n",
    "\n",
    "# Initialise the different lists with the data\n",
    "entities=[]\n",
    "texts=[]\n",
    "train_texts_entities = []\n",
    "\n",
    "# Iterate over all the different .xml files located in the specified directories\n",
    "for directory in train_dirs_whereto_parse:\n",
    "    \n",
    "    # Get the names of all the files in the directory and create a 'xml.root' object for\n",
    "    # each xml file\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in listdir(directory) if a.endswith('.xml')]\n",
    "    \n",
    "    # Iterate over all the different 'xml.root' objects to extract the needed information\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            train_texts_entities = train_texts_entities + [('START '+sentence.get('text')+' STOP', entities)]\n",
    "            entities =[]\n",
    "\n",
    "# train_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# Example: \n",
    "# [('I love Ibuprofeno and Frenadol', ['Ibuprofeno', 'Frenadol']), ('Give me a Fluimucil', ['Fluimucil'])]\n",
    "\n",
    "train_texts_entities[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\\n', ['testosterone', 'Plenaxis'])\n"
     ]
    }
   ],
   "source": [
    "## TESTING DATA\n",
    "\n",
    "# Same process as with the training data\n",
    "# In the testing data, for each sentance we have two related files:\n",
    "# - A file with a sentence to be parsed, in which we may encounter drug names (ending with 'text.txt')\n",
    "# - A file with the drug entities recognised in the sentence (ending with 'entities.txt')\n",
    "\n",
    "test_texts = []\n",
    "test_entities = []\n",
    "\n",
    "for directory in test_dirs_whereto_parse:\n",
    "    \n",
    "    # Si no poso el sorted, em llegeix els files amb un ordre aleatori.\n",
    "    # Amb el sorted m'asseguro que els corresponents files text.txt i entities.txt estan en la mateixa posicio\n",
    "    \n",
    "    # Read the pairs of files in alphabetical order\n",
    "    text_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('text.txt')])\n",
    "    entities_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('entities.txt')])\n",
    "    \n",
    "    for file in text_file_names:\n",
    "        file = open(file,'r')\n",
    "        test_texts = test_texts + [file.read()]\n",
    "        \n",
    "    for file in entities_file_names:\n",
    "        read_entities = []\n",
    "        with open(file,'r') as f:\n",
    "            for line in f:\n",
    "                read_entities = read_entities+[' '.join(line.split()[0:-1])] # separo en words, el.limino la ultima i torno a unir\n",
    "                \n",
    "        test_entities.append(read_entities)\n",
    "\n",
    "\n",
    "test_texts_entities=list(zip(test_texts,test_entities))\n",
    "\n",
    "\n",
    "# test_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there.\n",
    "print(test_texts_entities[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions that will be used in order to create the features\n",
    "def hasNumbers(string):\n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "def hasLetters(string):\n",
    "    return any(char.isalpha() for char in string)\n",
    "\n",
    "def hasUpperCase(string):\n",
    "    return any(char.isupper() for char in string)\n",
    "\n",
    "def allUpperCase(string):\n",
    "    return(string.isupper())\n",
    "\n",
    "def allLowerCase(string):\n",
    "    return(string.islower())\n",
    "\n",
    "def hasInitialCapital(string):\n",
    "    return(string[0].isupper())\n",
    "\n",
    "def containsSlash(string):\n",
    "    return('/' in string)\n",
    "\n",
    "def allLetters(string):\n",
    "    return(string.isalpha())\n",
    "\n",
    "def allDigits(string):\n",
    "    return(string.isdigit())\n",
    "\n",
    "def containsDash(string):\n",
    "    return('-' in string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the automatized creation of features given a tokenized sentence\n",
    "\n",
    "def feature_vector(sentence):\n",
    "    '''\n",
    "    Description:\n",
    "    \n",
    "    Examples/Tests:\n",
    "    \n",
    "    '''\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    #tokenized_sentence = ['START']+tokenized_sentence+['STOP']\n",
    "    # Feature: Initialise the feature_vector dictionary, in which we will create the features of each token\n",
    "    feature_vector = {}\n",
    "    \n",
    "    # Feature: Length of the token\n",
    "    feature_vector['token_length'] = [len(token) for token in tokenized_sentence]\n",
    "    \n",
    "    # Feature: Prefixes and Suffixes\n",
    "\n",
    "    prefix_feature = []\n",
    "    suffix_feature = []\n",
    "\n",
    "    prefixes = r'^meth|^eth|^prop|^but|^pent|^hex|^hept|^oct|^non|^dec'\n",
    "    suffixes = r'ane$|ene$|yne$|ol$|al$|amine$|cid$|ium$|ether$|ate$|one$'\n",
    "\n",
    "    for token in tokenized_sentence:\n",
    "\n",
    "            if re.search(prefixes,token):\n",
    "                prefix_feature=prefix_feature+[1]\n",
    "            else:\n",
    "                prefix_feature = prefix_feature+[0]\n",
    "\n",
    "            if re.search(suffixes,token):\n",
    "                suffix_feature=suffix_feature+[1]\n",
    "            else:\n",
    "                suffix_feature = suffix_feature+[0]\n",
    "\n",
    "    feature_vector['prefix_feature']=prefix_feature\n",
    "    feature_vector['suffix_feature']=suffix_feature\n",
    "\n",
    "    # Feature: Check if the token is already in the DrugBank database\n",
    "    \n",
    "    # Feature: POS of the token\n",
    "    \n",
    "    # recovering\n",
    "    # pos = nltk.pos_tag()\n",
    "    \n",
    "    # Feature: Binary token type features\n",
    "        # contains_hyphen, all_lowercase_letters, \n",
    "        # contains_slash, all_letters, contains_period, all_digits, contains_uppercase,\n",
    "        # contains_digit, contains_letters\n",
    "    \n",
    "    all_uppercase_letters = [1 if token.isupper() else 0 for token in tokenized_sentence]\n",
    "    all_lowercase_letters = [1 if token.islower() else 0 for token in tokenized_sentence]\n",
    "    initial_capital_letter = [1 if token[0].isupper() else 0 for token in tokenized_sentence]\n",
    "    contains_slash = [1 if '/' in token else 0 for token in tokenized_sentence]\n",
    "    all_letters = [1 if token.isalpha() else 0 for token in tokenized_sentence]\n",
    "    all_digits = [1 if token.isdigit() else 0 for token in tokenized_sentence]\n",
    "    contains_digit = [1 if hasNumbers(token) else 0 for token in tokenized_sentence]\n",
    "    contains_letters = [1 if hasLetters(token) else 0 for token in tokenized_sentence]\n",
    "    contains_uppercase = [1 if hasUpperCase(token) else 0 for token in tokenized_sentence]\n",
    "    contains_dash = [1 if '_' in token else 0 for token in tokenized_sentence]\n",
    "    \n",
    "    feature_vector['all_uppercase_letters']=all_uppercase_letters\n",
    "    feature_vector['all_lowercase_letters']=all_lowercase_letters\n",
    "    feature_vector['initial_capital_letter']=initial_capital_letter\n",
    "    feature_vector['contains_slash']=contains_slash\n",
    "    feature_vector['all_letters']=all_uppercase_letters\n",
    "    feature_vector['all_digits']=all_digits\n",
    "    feature_vector['contains_digit']=contains_digit\n",
    "    feature_vector['contains_letters']=contains_letters\n",
    "    feature_vector['contains_uppercase']=contains_uppercase  \n",
    "    feature_vector['contains_dash']=contains_dash  \n",
    "    \n",
    "    \n",
    "    # Feature: Position of the token in the sentence (distance from the 'START' token)\n",
    "    idx_position = []\n",
    "    current_position = -1\n",
    "    for token in tokenized_sentence:\n",
    "        if token == 'STOP':\n",
    "            current_position = -1\n",
    "            idx_position.append(current_position)\n",
    "        else:\n",
    "            current_position += 1\n",
    "            idx_position.append(current_position)\n",
    "    feature_vector['idx_position'] = idx_position\n",
    "    \n",
    "    \n",
    "    # Feature: Binary token type features of the +-2 previous/following tokens\n",
    "    \n",
    "    for k in [-2, -1, 1, 2]:\n",
    "        all_uppercase_letters_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = allUpperCase)\n",
    "        all_lowercase_letters_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = allLowerCase)\n",
    "        initial_capital_letter_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = hasInitialCapital)\n",
    "        contains_slash_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = containsSlash)\n",
    "        all_letters_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = allLetters)\n",
    "        all_digits_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = allDigits)\n",
    "        contains_digit_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = hasNumbers)\n",
    "        contains_letters_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = hasLetters)\n",
    "        contains_uppercase_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = hasUpperCase)\n",
    "        contains_dash_context = checkPreviousTokenCondition(tokens = tokenized_sentence, \n",
    "                                                                 pos = k, condition = containsDash)\n",
    "        \n",
    "        feature_vector['all_uppercase_letters_context%d' % k] = all_uppercase_letters_context\n",
    "        feature_vector['all_lowercase_letters_context%d' % k] = all_lowercase_letters_context\n",
    "        feature_vector['initial_capital_letter_context%d' % k] = initial_capital_letter_context\n",
    "        feature_vector['contains_slash_context%d' % k] = contains_slash_context\n",
    "        feature_vector['all_letters_context%d' % k] = all_letters_context\n",
    "        feature_vector['all_digits_context%d' % k] = all_digits_context\n",
    "        feature_vector['contains_digit_context%d' % k] = contains_digit_context\n",
    "        feature_vector['contains_letters_context%d' % k] = contains_letters_context\n",
    "        feature_vector['contains_uppercase_context%d' % k] = contains_uppercase_context\n",
    "        feature_vector['contains_dash_context%d' % k] = contains_dash_context\n",
    "    \n",
    "    \n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO Tagger\n",
    "\n",
    "In this section we will tag each sentence with the BIO format. For this, we have created a function called 'bioTagger' which will perform the following actions:\n",
    "\n",
    "Given a sentence 'text' and a set of drugs 'drugs', this function returns a list of str that\n",
    "contains a tag for each of the tokens in text. The tags can be either 'B', 'I' or 'O'. 'B' means\n",
    "the token is the first part of a drug entity, 'I' means the token is the continuation of a drug entity,\n",
    "and 'O' means that the token does not belong to a drug entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the needed lists\n",
    "tokens = []\n",
    "tags = []\n",
    "features = pd.DataFrame()\n",
    "# Iterate over all the train entities (tuples of (sentence, drugs)) and apply the bioTagger function\n",
    "for text,drugs in train_texts_entities:\n",
    "    features = pd.concat([features,pd.DataFrame(feature_vector(text))])\n",
    "    tuples = bioTagger(text, drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "    \n",
    "# Create a training set with the tokens and the BIO tags\n",
    "train_df = features\n",
    "train_df['token']=tokens\n",
    "train_df['output']=tags\n",
    "#train_set = {'token':tokens,'output':tags}\n",
    "#train_df = pd.DataFrame(train_set)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_digits</th>\n",
       "      <th>all_digits_context-1</th>\n",
       "      <th>all_digits_context-2</th>\n",
       "      <th>all_digits_context1</th>\n",
       "      <th>all_digits_context2</th>\n",
       "      <th>all_letters</th>\n",
       "      <th>all_letters_context-1</th>\n",
       "      <th>all_letters_context-2</th>\n",
       "      <th>all_letters_context1</th>\n",
       "      <th>all_letters_context2</th>\n",
       "      <th>...</th>\n",
       "      <th>initial_capital_letter</th>\n",
       "      <th>initial_capital_letter_context-1</th>\n",
       "      <th>initial_capital_letter_context-2</th>\n",
       "      <th>initial_capital_letter_context1</th>\n",
       "      <th>initial_capital_letter_context2</th>\n",
       "      <th>prefix_feature</th>\n",
       "      <th>suffix_feature</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>START</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Formal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>drug</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>interaction</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>studies</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   all_digits  all_digits_context-1  all_digits_context-2  \\\n",
       "0           0                     0                     0   \n",
       "1           0                     0                     0   \n",
       "2           0                     0                     0   \n",
       "3           0                     0                     0   \n",
       "4           0                     0                     0   \n",
       "\n",
       "   all_digits_context1  all_digits_context2  all_letters  \\\n",
       "0                    0                    0            1   \n",
       "1                    0                    0            0   \n",
       "2                    0                    0            0   \n",
       "3                    0                    0            0   \n",
       "4                    0                    0            0   \n",
       "\n",
       "   all_letters_context-1  all_letters_context-2  all_letters_context1  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     1   \n",
       "2                      1                      0                     1   \n",
       "3                      1                      1                     1   \n",
       "4                      1                      1                     1   \n",
       "\n",
       "   all_letters_context2   ...    initial_capital_letter  \\\n",
       "0                     0   ...                         1   \n",
       "1                     1   ...                         1   \n",
       "2                     1   ...                         0   \n",
       "3                     1   ...                         0   \n",
       "4                     1   ...                         0   \n",
       "\n",
       "   initial_capital_letter_context-1  initial_capital_letter_context-2  \\\n",
       "0                                 0                                 0   \n",
       "1                                 0                                 0   \n",
       "2                                 1                                 0   \n",
       "3                                 0                                 1   \n",
       "4                                 0                                 0   \n",
       "\n",
       "   initial_capital_letter_context1  initial_capital_letter_context2  \\\n",
       "0                                0                                0   \n",
       "1                                0                                0   \n",
       "2                                0                                0   \n",
       "3                                0                                0   \n",
       "4                                0                                0   \n",
       "\n",
       "   prefix_feature  suffix_feature  token_length        token  output  \n",
       "0               0               0             5        START       O  \n",
       "1               0               1             6       Formal       O  \n",
       "2               0               0             4         drug       O  \n",
       "3               0               0            11  interaction       O  \n",
       "4               0               0             7      studies       O  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "X = train_df.loc[:, [all(x) for x in list(zip(train_df.columns!=target_name,train_df.columns!=token_name))]]\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   all_digits  all_digits_context-1  all_digits_context-2  \\\n",
      "0           0                     0                     0   \n",
      "1           0                     0                     0   \n",
      "2           0                     0                     0   \n",
      "3           0                     0                     0   \n",
      "4           0                     0                     0   \n",
      "\n",
      "   all_digits_context1  all_digits_context2  all_letters  \\\n",
      "0                    0                    0            1   \n",
      "1                    0                    0            0   \n",
      "2                    0                    0            0   \n",
      "3                    0                    0            0   \n",
      "4                    0                    0            0   \n",
      "\n",
      "   all_letters_context-1  all_letters_context-2  all_letters_context1  \\\n",
      "0                      0                      0                     0   \n",
      "1                      0                      0                     1   \n",
      "2                      1                      0                     1   \n",
      "3                      1                      1                     1   \n",
      "4                      1                      1                     1   \n",
      "\n",
      "   all_letters_context2      ...       contains_uppercase_context2  \\\n",
      "0                     0      ...                                 0   \n",
      "1                     1      ...                                 0   \n",
      "2                     1      ...                                 0   \n",
      "3                     1      ...                                 0   \n",
      "4                     1      ...                                 0   \n",
      "\n",
      "   idx_position  initial_capital_letter  initial_capital_letter_context-1  \\\n",
      "0             0                       1                                 0   \n",
      "1             1                       1                                 0   \n",
      "2             2                       0                                 1   \n",
      "3             3                       0                                 0   \n",
      "4             4                       0                                 0   \n",
      "\n",
      "   initial_capital_letter_context-2  initial_capital_letter_context1  \\\n",
      "0                                 0                                0   \n",
      "1                                 0                                0   \n",
      "2                                 0                                0   \n",
      "3                                 1                                0   \n",
      "4                                 0                                0   \n",
      "\n",
      "   initial_capital_letter_context2  prefix_feature  suffix_feature  \\\n",
      "0                                0               0               0   \n",
      "1                                0               0               1   \n",
      "2                                0               0               0   \n",
      "3                                0               0               0   \n",
      "4                                0               0               0   \n",
      "\n",
      "   token_length  \n",
      "0             5  \n",
      "1             6  \n",
      "2             4  \n",
      "3            11  \n",
      "4             7  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "0    O\n",
      "1    O\n",
      "2    O\n",
      "3    O\n",
      "4    O\n",
      "Name: output, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning SVM in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for GridSearchCV:  0.0009331703186035156\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM object with the corresponding tunned parameters\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Look for the best parameters of the SVM model with GridSearchCV\n",
    "start = time.time()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "end = time.time()\n",
    "print('Execution time for GridSearchCV: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time of the SVM:  3.8438780307769775\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model with the parameters selected before\n",
    "start = time.time()\n",
    "clf.fit(X,Y)\n",
    "end = time.time()\n",
    "print('Training time of the SVM: ', str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with just one test text. Let's tokenize it, and create its feature vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  No drug, nutritional supplement, food or herb interactions have yet been reported.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  [] \n",
      "\n",
      "text:  No formal drug/drug interaction studies with Plenaxis were performed.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis'] \n",
      "\n",
      "text:  Cytochrome P-450 is not known to be involved in the metabolism of Plenaxis.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  [] \n",
      "\n",
      "text:  Plenaxis is highly bound to plasma proteins (96 to 99%).\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['proteins'] \n",
      "\n",
      "text:  Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\n",
      "\n",
      "real entities:  ['testosterone', 'Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'B' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis', 'total', 'testosterone'] \n",
      "\n",
      "text:  Serum transaminase levels should be obtained before starting treatment with Plenaxis and periodically during treatment.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis'] \n",
      "\n",
      "text:  Periodic measurement of serum PSA levels may also be considered.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['PSA'] \n",
      "\n",
      "text:  Formal drug interaction studies have not been conducted with ORENCIA.\n",
      "\n",
      "real entities:  ['ORENCIA'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['Formal'] \n",
      "\n",
      "text:  Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance.\n",
      "\n",
      "real entities:  ['abatacept', 'corticosteroids', 'TNF blocking agents', 'MTX', 'NSAIDs'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['MTX', 'NSAIDs', 'corticosteroids'] \n",
      "\n",
      "text:  The majority of patients in RA clinical studies received one or more of the following concomitant medications with ORENCIA: MTX, NSAIDs, corticosteroids, TNF blocking agents, azathioprine, chloroquine, gold, hydroxychloroquine, leflunomide, sulfasalazine, and anakinra.\n",
      "\n",
      "real entities:  ['anakinra', 'sulfasalazine', 'corticosteroids', 'NSAIDs', 'MTX', 'ORENCIA', 'gold', 'chloroquine', 'azathioprine', 'TNF blocking agents', 'leflunomide', 'hydroxychloroquine'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O'\n",
      " 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['RA', 'one', 'ORENCIA', 'MTX', 'NSAIDs', 'corticosteroids', 'TNF', 'agents', 'azathioprine', 'chloroquine', 'gold', 'hydroxychloroquine', 'leflunomide', 'sulfasalazine'] \n",
      "\n",
      "text:  Concurrent administration of a TNF antagonist with ORENCIA has been associated with an increased risk of serious infections and no significant additional efficacy over use of the TNF antagonists alone.\n",
      "\n",
      "real entities:  ['TNF antagonist', 'ORENCIA', 'TNF antagonists'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['TNF', 'ORENCIA', 'additional', 'TNF'] \n",
      "\n",
      "text:  Concurrent therapy with ORENCIA and TNF antagonists is not recommended.\n",
      "\n",
      "real entities:  ['TNF antagonists', 'ORENCIA'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA', 'TNF'] \n",
      "\n",
      "text:  There is insufficient experience to assess the safety and efficacy of ORENCIA administered concurrently with anakinra, and therefore such use is not recommended.\n",
      "\n",
      "real entities:  ['ORENCIA', 'anakinra'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'B' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA', 'anakinra'] \n",
      "\n",
      "text:  Formal drug interaction studies with Abciximab have not been conducted.\n",
      "\n",
      "real entities:  ['Abciximab'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Formal', 'Abciximab'] \n",
      "\n",
      "text:  Abciximab has been administered to patients with ischemic heart disease treated concomitantly with a broad range of medications used in the treatment of angina myocardial infarction and hypertension.\n",
      "\n",
      "real entities:  ['Abciximab'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['myocardial'] \n",
      "\n",
      "text:  These medications have included heparin, warfarin, beta-adrenergic receptor blockers, calcium channel antagonists, angiotensin converting enzyme inhibitors, intravenous and oral nitrates, ticlopidine, and aspirin.\n",
      "\n",
      "real entities:  ['aspirin', 'ticlopidine', 'nitrates', 'angiotensin converting enzyme inhibitors', 'calcium channel antagonists', 'beta-adrenergic receptor blockers', 'warfarin', 'heparin'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'I' 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O'\n",
      " 'O' 'B' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the tags was not recognised. Please check the \"bio_tags\" parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dc1b18a96e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predicted bio tags: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpred_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbioTagsToEntities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbio_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predicted entities: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data Science/Github/drug-interactions/drug_functions.py\u001b[0m in \u001b[0;36mbioTagsToEntities\u001b[0;34m(tokens, bio_tags)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# Any other case should raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'One of the tags was not recognised. Please check the \"bio_tags\" parameter.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the tags was not recognised. Please check the \"bio_tags\" parameter."
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text,entities in test_texts_entities:\n",
    "    print('text: ', text)\n",
    "    print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_tags = clf.predict(pd.DataFrame(feature_vector(text)))\n",
    "    predictions.append((list(predicted_tags),entities,text)) \n",
    "    print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    pred_entities = bioTagsToEntities(tokens = tokens, bio_tags = predicted_tags)\n",
    "    print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "# print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define a function that recover's the whole drug name from BIO taggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be based on $$F1=\\frac{2*precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest exemple m'ha ajudat a entendre com calcular la precision i la recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "true = ['hola','que','ca','bo']\n",
    "pred = ['hola','que','pet']\n",
    "\n",
    "# Precision\n",
    "print(round(len([word for word in pred if word in true])/len(pred),2))\n",
    "\n",
    "# Recall\n",
    "print(round(len([word for word in pred if word in true])/len(true),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(pred_ent),2)*100     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(true_ent),2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover all the words from the predicted bio_tags and try to compute F1 for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the tags was not recognised. Please check the \"bio_tags\" parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3c947d6405cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# I need the tokens for the bioTagsToEntities function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredicted_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbioTagsToEntities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompute_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_entities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompute_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_entities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data Science/Github/drug-interactions/drug_functions.py\u001b[0m in \u001b[0;36mbioTagsToEntities\u001b[0;34m(tokens, bio_tags)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# Any other case should raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'One of the tags was not recognised. Please check the \"bio_tags\" parameter.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the tags was not recognised. Please check the \"bio_tags\" parameter."
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "precision = []\n",
    "recall = []\n",
    "for tags, true_entities, text in predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = bioTagsToEntities(tokens,tags)\n",
    "    precision = precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    recall = recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "print('precision: ',avg_precision)\n",
    "print('recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1 = (2*avg_precision*avg_recall) / (avg_precision + avg_recall)\n",
    "print('F1: ', F1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
