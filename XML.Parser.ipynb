{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Load needed modules and specify the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "from lxml import etree # XML file parsing\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV # Parameter selection\n",
    "import time # Execution time of some blocks\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import statistics\n",
    "\n",
    "# Import our defined functions\n",
    "from drug_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the data directories\n",
    "train_dirs_whereto_parse = ['data/small_train_DrugBank']\n",
    "test_dirs_whereto_parse = ['data/small_test_DrugBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train and test data from the XML files\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the token 'STOP' at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('START Formal drug interaction studies have not been conducted with ORENCIA. STOP',\n",
       "  ['ORENCIA']),\n",
       " ('START Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance. STOP',\n",
       "  ['MTX', 'NSAIDs', 'corticosteroids', 'TNF blocking agents', 'abatacept'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRAINING DATA\n",
    "\n",
    "# Initialise the different lists with the data\n",
    "entities=[]\n",
    "texts=[]\n",
    "train_texts_entities = []\n",
    "\n",
    "# Iterate over all the different .xml files located in the specified directories\n",
    "for directory in train_dirs_whereto_parse:\n",
    "    \n",
    "    # Get the names of all the files in the directory and create a 'xml.root' object for\n",
    "    # each xml file\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in listdir(directory) if a.endswith('.xml')]\n",
    "    \n",
    "    # Iterate over all the different 'xml.root' objects to extract the needed information\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            train_texts_entities = train_texts_entities + [('START '+sentence.get('text')+' STOP', entities)]\n",
    "            entities =[]\n",
    "\n",
    "# train_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# Example: \n",
    "# [('I love Ibuprofeno and Frenadol', ['Ibuprofeno', 'Frenadol']), ('Give me a Fluimucil', ['Fluimucil'])]\n",
    "\n",
    "train_texts_entities[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\\n', ['testosterone', 'Plenaxis'])\n"
     ]
    }
   ],
   "source": [
    "## TESTING DATA\n",
    "\n",
    "# Same process as with the training data\n",
    "# In the testing data, for each sentance we have two related files:\n",
    "# - A file with a sentence to be parsed, in which we may encounter drug names (ending with 'text.txt')\n",
    "# - A file with the drug entities recognised in the sentence (ending with 'entities.txt')\n",
    "\n",
    "test_texts = []\n",
    "test_entities = []\n",
    "\n",
    "for directory in test_dirs_whereto_parse:\n",
    "    \n",
    "    # Si no poso el sorted, em llegeix els files amb un ordre aleatori.\n",
    "    # Amb el sorted m'asseguro que els corresponents files text.txt i entities.txt estan en la mateixa posicio\n",
    "    \n",
    "    # Read the pairs of files in alphabetical order\n",
    "    text_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('text.txt')])\n",
    "    entities_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('entities.txt')])\n",
    "    \n",
    "    for file in text_file_names:\n",
    "        file = open(file,'r')\n",
    "        test_texts = test_texts + [file.read()]\n",
    "        \n",
    "    for file in entities_file_names:\n",
    "        read_entities = []\n",
    "        with open(file,'r') as f:\n",
    "            for line in f:\n",
    "                read_entities = read_entities+[' '.join(line.split()[0:-1])] # separo en words, el.limino la ultima i torno a unir\n",
    "                \n",
    "        test_entities.append(read_entities)\n",
    "\n",
    "\n",
    "test_texts_entities=list(zip(test_texts,test_entities))\n",
    "\n",
    "\n",
    "# test_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there.\n",
    "print(test_texts_entities[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO Tagger and Feature Creation\n",
    "\n",
    "In this section we will tag each sentence with the BIO format. For this, we have created a function called 'BIOTagger' which will perform the following actions:\n",
    "\n",
    "Given a sentence 'text' and a set of drugs 'drugs', this function returns a list of str that\n",
    "contains a tag for each of the tokens in text. The tags can be either 'B', 'I' or 'O'. 'B' means\n",
    "the token is the first part of a drug entity, 'I' means the token is the continuation of a drug entity,\n",
    "and 'O' means that the token does not belong to a drug entity.\n",
    "\n",
    "Apart from that, we have also downloaded the DrugBank database (ref: https://www.drugbank.ca/) from we will extract all the named entities. We will create a list out of these set of entities and for each token processed, we will check if the token is already in the database, meaning that has a very high probability of being a NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START', 'Formal', 'drug', 'interaction', 'studies', 'have', 'not', 'been', 'conducted', 'with', 'ORENCIA', '.', 'STOP', 'START', 'Population', 'pharmacokinetic', 'analyses', 'revealed', 'that', 'MTX', ',', 'NSAIDs', ',', 'corticosteroids', ',', 'and', 'TNF', 'blocking', 'agents', 'did', 'not', 'influence', 'abatacept', 'clearance', '.', 'STOP', 'START', 'The', 'majority', 'of', 'patients', 'in', 'RA', 'clinical', 'studies', 'received', 'one', 'or', 'more', 'of', 'the', 'following', 'concomitant', 'medications', 'with', 'ORENCIA', ':', 'MTX', ',', 'NSAIDs', ',', 'corticosteroids', ',', 'TNF', 'blocking', 'agents', ',', 'azathioprine', ',', 'chloroquine', ',', 'gold', ',', 'hydroxychloroquine', ',', 'leflunomide', ',', 'sulfasalazine', ',', 'and', 'anakinra', '.', 'STOP', 'START', 'Concurrent', 'administration', 'of', 'a', 'TNF', 'antagonist', 'with', 'ORENCIA', 'has', 'been', 'associated', 'with', 'an', 'increased', 'risk', 'of', 'serious', 'infections', 'and', 'no', 'significant', 'additional', 'efficacy', 'over', 'use', 'of', 'the', 'TNF', 'antagonists', 'alone', '.', 'STOP', 'START', 'Concurrent', 'therapy', 'with', 'ORENCIA', 'and', 'TNF', 'antagonists', 'is', 'not', 'recommended', '.', 'STOP', 'START', 'There', 'is', 'insufficient', 'experience', 'to', 'assess', 'the', 'safety', 'and', 'efficacy', 'of', 'ORENCIA', 'administered', 'concurrently', 'with', 'anakinra', ',', 'and', 'therefore', 'such', 'use', 'is', 'not', 'recommended', '.', 'STOP', 'START', 'The', 'concomitant', 'intake', 'of', 'alcohol', 'and', 'Acamprosate', 'does', 'not', 'affect', 'the', 'pharmacokinetics', 'of', 'either', 'alcohol', 'or', 'acamprosate', '.', 'STOP', 'START', 'Pharmacokinetic', 'studies', 'indicate', 'that', 'administration', 'of', 'disulfiram', 'or', 'diazepam', 'does', 'not', 'affect', 'the', 'pharmacokinetics', 'of', 'acamprosate', '.', 'STOP', 'START', 'Co', 'administration', 'of', 'naltrexone', 'with', 'Acamprosate', 'produced', 'a', '25', '%', 'increase', 'in', 'AUC', 'and', 'a', '33', '%', 'increase', 'in', 'the', 'Cmax', 'of', 'acamprosate', '.', 'STOP', 'START', 'No', 'adjustment', 'of', 'dosage', 'is', 'recommended', 'in', 'such', 'patients', '.', 'STOP', 'START', 'The', 'pharmacokinetics', 'of', 'naltrexone', 'and', 'its', 'major', 'metabolite', '6', 'beta', 'naltrexol', 'were', 'unaffected', 'following', 'co', 'administration', 'with', 'Acamprosate', '.', 'STOP', 'START', 'Other', 'concomitant', 'therapies', ':', 'In', 'clinical', 'trials', ',', 'the', 'safety', 'profile', 'in', 'subjects', 'treated', 'with', 'Acamprosate', 'concomitantly', 'with', 'anxiolytics', ',', 'hypnotics', 'and', 'sedatives', '(', 'including', 'benzodiazepines', ')', ',', 'or', 'non', 'opioid', 'analgesics', 'was', 'similar', 'to', 'that', 'of', 'subjects', 'taking', 'placebo', 'with', 'these', 'concomitant', 'medications', '.', 'STOP', 'START', 'Patients', 'taking', 'Acamprosate', 'concomitantly', 'with', 'antidepressants', 'more', 'commonly', 'reported', 'both', 'weight', 'gain', 'and', 'weight', 'loss', ',', 'compared', 'with', 'patients', 'taking', 'either', 'medication', 'alone', '.', 'STOP', 'START', 'Formal', 'drug', 'interaction', 'studies', 'with', 'Abciximab', 'have', 'not', 'been', 'conducted', '.', 'STOP', 'START', 'Abciximab', 'has', 'been', 'administered', 'to', 'patients', 'with', 'ischemic', 'heart', 'disease', 'treated', 'concomitantly', 'with', 'a', 'broad', 'range', 'of', 'medications', 'used', 'in', 'the', 'treatment', 'of', 'angina', 'myocardial', 'infarction', 'and', 'hypertension', '.', 'STOP', 'START', 'These', 'medications', 'have', 'included', 'heparin', ',', 'warfarin', ',', 'beta', 'adrenergic', 'receptor', 'blockers', ',', 'calcium', 'channel', 'antagonists', ',', 'angiotensin', 'converting', 'enzyme', 'inhibitors', ',', 'intravenous', 'and', 'oral', 'nitrates', ',', 'ticlopidine', ',', 'and', 'aspirin', '.', 'STOP', 'START', 'Heparin', ',', 'other', 'anticoagulants', ',', 'thrombolytics', ',', 'and', 'anti', 'platelet', 'agents', 'are', 'associated', 'with', 'an', 'increase', 'in', 'bleeding', '.', 'STOP', 'START', 'Patients', 'with', 'HACA', 'titers', 'may', 'have', 'allergic', 'or', 'hypersensitivity', 'reactions', 'when', 'treated', 'with', 'other', 'diagnostic', 'or', 'therapeutic', 'monoclonal', 'antibodies', '.', 'STOP', 'START', 'Certain', 'drugs', 'tend', 'to', 'produce', 'hyperglycemia', 'and', 'may', 'lead', 'to', 'loss', 'of', 'blood', 'glucose', 'control', '.', 'STOP', 'START', 'These', 'drugs', 'include', 'the', 'thiazides', 'and', 'other', 'diuretics', ',', 'corticosteroids', ',', 'phenothiazines', ',', 'thyroid', 'products', ',', 'estrogens', ',', 'oral', 'contraceptives', ',', 'phenytoin', ',', 'nicotinic', 'acid', ',', 'sympathomimetics', ',', 'calcium', 'channel', 'blocking', 'drugs', ',', 'and', 'isoniazid', '.', 'STOP', 'START', 'When', 'such', 'drugs', 'are', 'administered', 'to', 'a', 'patient', 'receiving', 'Acarbose', ',', 'the', 'patient', 'should', 'be', 'closely', 'observed', 'for', 'loss', 'of', 'blood', 'glucose', 'control', '.', 'STOP', 'START', 'When', 'such', 'drugs', 'are', 'withdrawn', 'from', 'patients', 'receiving', 'Acarbose', 'in', 'combination', 'with', 'sulfonylureas', 'or', 'insulin', ',', 'patients', 'should', 'be', 'observed', 'closely', 'for', 'any', 'evidence', 'of', 'hypoglycemia', '.', 'STOP', 'START', 'Intestinal', 'adsorbents', '(', 'e.', 'g.', ',', 'charcoal', ')', 'and', 'digestive', 'enzyme', 'preparations', 'containing', 'carbohydrate', 'splitting', 'enzymes', '(', 'e.', 'g.', ',', 'amylase', ',', 'pancreatin', ')', 'may', 'reduce', 'the', 'effect', 'of', 'Acarbose', 'and', 'should', 'not', 'be', 'taken', 'concomitantly', '.', 'STOP', 'START', 'Acarbose', 'has', 'been', 'shown', 'to', 'change', 'the', 'bioavailabillty', 'digoxin', 'when', 'they', 'are', 'co', 'administered', ',', 'which', 'may', 'require', 'digoxin', 'dose', 'adjustment', '.', 'STOP', 'START', 'Studies', 'in', 'healthy', 'volunteers', 'have', 'shown', 'that', 'Acarbose', 'has', 'no', 'effect', 'on', 'either', 'the', 'pharmacokinetics', 'or', 'pharmacodynamics', 'of', 'digoxin', ',', 'nifedipine', ',', 'propranolol', ',', 'or', 'ranitidine', '.', 'STOP', 'START', 'Acarbose', 'did', 'not', 'interfere', 'with', 'the', 'absorption', 'or', 'disposition', 'of', 'the', 'sulfonylurea', 'glyburide', 'in', 'diabetic', 'patients', '.', 'STOP', 'START', 'Acarbose', 'may', 'affect', 'digoxin', 'bioavailabillty', 'and', 'may', 'require', 'dose', 'adjustment', 'of', 'digoxin', 'by', '16', '%', '(', '90', '%', 'confidence', 'interval', ':', '8', '23', '%', ')', ',', 'decrease', 'mean', 'C', 'max', 'digoxin', 'by', '26', '%', '(', '90', '%', 'confidence', 'interval', ':', '16', '34', '%', ')', 'and', 'decrease', 'mean', 'trough', 'concentrations', 'of', 'digoxin', 'by', '9', '%', '(', '90', '%', 'confidence', 'limit', ':', '19', '%', 'decrease', 'to', '2', '%', 'increase', ')', '.', 'STOP', 'START', 'The', 'amount', 'of', 'metformin', 'absorbed', 'while', 'taking', 'Acarbose', 'was', 'bioequivalent', 'to', 'the', 'amount', 'absorbed', 'when', 'taking', 'placebo', ',', 'as', 'indicated', 'by', 'the', 'plasma', 'AUC', 'values', '.', 'STOP', 'START', 'However', ',', 'the', 'peak', 'plasma', 'level', 'of', 'metformin', 'was', 'reduced', 'by', 'approximately', '20', '%', 'when', 'taking', 'Acarbose', 'due', 'to', 'a', 'slight', 'delay', 'in', 'the', 'absorption', 'of', 'metformin', '.', 'STOP', 'START', 'There', 'is', 'little', 'if', 'any', 'clinically', 'significant', 'interaction', 'between', 'Acarbose', 'and', 'metformin', '.', 'STOP', 'START', 'Concomitant', 'use', 'with', 'iron', 'supplements', 'may', 'result', 'in', 'the', 'reduced', 'absorption', 'of', 'iron', '.', 'STOP', 'START', 'No', 'formal', 'drug/drug', 'interaction', 'studies', 'with', 'Plenaxis', 'were', 'performed', '.', 'STOP', 'START', 'Cytochrome', 'P', '450', 'is', 'not', 'known', 'to', 'be', 'involved', 'in', 'the', 'metabolism', 'of', 'Plenaxis', '.', 'STOP', 'START', 'Plenaxis', 'is', 'highly', 'bound', 'to', 'plasma', 'proteins', '(', '96', 'to', '99', '%', ')', '.', 'STOP', 'START', 'Laboratory', 'Tests', 'Response', 'to', 'Plenaxis', 'should', 'be', 'monitored', 'by', 'measuring', 'serum', 'total', 'testosterone', 'concentrations', 'just', 'prior', 'to', 'administration', 'on', 'Day', '29', 'and', 'every', '8', 'weeks', 'thereafter', '.', 'STOP', 'START', 'Serum', 'transaminase', 'levels', 'should', 'be', 'obtained', 'before', 'starting', 'treatment', 'with', 'Plenaxis', 'and', 'periodically', 'during', 'treatment', '.', 'STOP', 'START', 'Periodic', 'measurement', 'of', 'serum', 'PSA', 'levels', 'may', 'also', 'be', 'considered', '.', 'STOP', 'START', 'Catecholamine', 'depleting', 'drugs', ',', 'such', 'as', 'reserpine', ',', 'may', 'have', 'an', 'additive', 'effect', 'when', 'given', 'with', 'beta', 'blocking', 'agents', '.', 'STOP', 'START', 'Patients', 'treated', 'with', 'acebutolol', 'plus', 'catecholamine', 'depletors', 'should', ',', 'therefore', ',', 'be', 'observed', 'closely', 'for', 'evidence', 'of', 'marked', 'bradycardia', 'or', 'hypotension', 'which', 'may', 'present', 'as', 'vertigo', ',', 'syncope/presyncope', ',', 'or', 'orthostatic', 'changes', 'in', 'blood', 'pressure', 'without', 'compensatory', 'tachycardia', '.', 'STOP', 'START', 'Exaggerated', 'hypertensive', 'responses', 'have', 'been', 'reported', 'from', 'the', 'combined', 'use', 'of', 'beta', 'adrenergic', 'antagonists', 'and', 'alpha', 'adrenergic', 'stimulants', ',', 'including', 'those', 'contained', 'in', 'proprietary', 'cold', 'remedies', 'and', 'vasoconstrictive', 'nasal', 'drops', '.', 'STOP', 'START', 'Patients', 'receiving', 'beta', 'blockers', 'should', 'be', 'warned', 'of', 'this', 'potential', 'hazard', '.', 'STOP', 'START', 'Blunting', 'of', 'the', 'antihypertensive', 'effect', 'of', 'beta', 'adrenoceptor', 'blocking', 'agents', 'by', 'nonsteroidal', 'anti', 'inflammatory', 'drugs', 'has', 'been', 'reported', '.', 'STOP', 'START', 'No', 'significant', 'interactions', 'with', 'digoxin', ',', 'hydrochlorothiazide', ',', 'hydralazine', ',', 'sulfinpyrazone', ',', 'oral', 'contraceptives', ',', 'tolbutamide', ',', 'or', 'warfarin', 'have', 'been', 'observed', '.', 'STOP', 'START', 'Co', 'administration', 'of', 'probenecid', 'with', 'acyclovir', 'has', 'been', 'shown', 'to', 'increase', 'the', 'mean', 'half', 'life', 'and', 'the', 'area', 'under', 'the', 'concentration', 'time', 'curve', '.', 'STOP', 'START', 'Urinary', 'excretion', 'and', 'renal', 'clearance', 'were', 'correspondingly', 'reduced', '.', 'STOP', 'START', 'The', 'clinical', 'effects', 'of', 'this', 'combination', 'have', 'not', 'been', 'studied', '.', 'STOP', 'START', 'DIAMOX', 'modifies', 'phenytoin', 'metabolism', 'with', 'increased', 'serum', 'levels', 'of', 'phenytoin', '.', 'STOP', 'START', 'This', 'may', 'increase', 'or', 'enhance', 'the', 'occurrence', 'of', 'osteomalacia', 'in', 'some', 'patients', 'receiving', 'chronic', 'phenytoin', 'therapy', '.', 'STOP', 'START', 'Caution', 'is', 'advised', 'in', 'patients', 'receiving', 'chronic', 'concomitant', 'therapy', '.', 'STOP', 'START', 'By', 'decreasing', 'the', 'gastrointestinal', 'absorption', 'of', 'primidone', ',', 'DIAMOX', 'may', 'decrease', 'serum', 'concentrations', 'of', 'primidone', 'and', 'its', 'metabolites', ',', 'with', 'a', 'consequent', 'possible', 'decrease', 'in', 'anticonvulsant', 'effect', '.', 'STOP', 'START', 'Caution', 'is', 'advised', 'when', 'beginning', ',', 'discontinuing', ',', 'or', 'changing', 'the', 'dose', 'of', 'DIAMOX', 'in', 'patients', 'receiving', 'primidone', '.', 'STOP', 'START', 'Because', 'of', 'possible', 'additive', 'effects', 'with', 'other', 'carbonic', 'anhydrase', 'inhibitors', ',', 'concomitant', 'use', 'is', 'not', 'advisable', '.', 'STOP', 'START', 'Acetazolamide', 'may', 'increase', 'the', 'effects', 'of', 'other', 'folic', 'acid', 'antagonists', '.', 'STOP', 'START', 'Acetazolamide', 'may', 'increase', 'or', 'decrease', 'blood', 'glucose', 'levels', '.', 'STOP', 'START', 'Consideration', 'should', 'be', 'taken', 'in', 'patients', 'being', 'treated', 'with', 'antidiabetic', 'agents', '.', 'STOP', 'START', 'Acetazolamide', 'decreases', 'urinary', 'excretion', 'of', 'amphetamine', 'and', 'may', 'enhance', 'the', 'magnitude', 'and', 'duration', 'of', 'their', 'effect', '.', 'STOP', 'START', 'Acetazolamide', 'reduces', 'urinary', 'excretion', 'of', 'quinidine', 'and', 'may', 'enhance', 'its', 'effect', '.', 'STOP', 'START', 'Acetazolamide', 'may', 'prevent', 'the', 'urinary', 'antiseptic', 'effect', 'of', 'methenamine', '.', 'STOP', 'START', 'Acetazolamide', 'increases', 'lithium', 'excretion', 'and', 'the', 'lithium', 'may', 'be', 'decreased', '.', 'STOP', 'START', 'Acetazolamide', 'and', 'sodium', 'bicarbonate', 'used', 'concurrently', 'increases', 'the', 'risk', 'of', 'renal', 'calculus', 'formation', '.', 'STOP', 'START', 'Acetazolamide', 'may', 'elevate', 'cyclosporine', 'levels', '.', 'STOP', 'START', 'No', 'drug', ',', 'nutritional', 'supplement', ',', 'food', 'or', 'herb', 'interactions', 'have', 'yet', 'been', 'reported', '.', 'STOP']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'B', 'B', 'O', 'B', 'B', 'B', 'O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'O', 'B', 'B', 'B', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-19f49e678f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2568\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2879\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "# Load the DrugBank list of entities (it has already been processed for the extraction of the NE).\n",
    "# Each line of the file contains a different named entity.\n",
    "with(open('data/DrugBank_names_DB.txt', 'r')) as f:\n",
    "    drugbank_db = f.read().splitlines()\n",
    "        \n",
    "# Initialise the needed lists\n",
    "tokens = []\n",
    "tags = []\n",
    "removed_columns = []\n",
    "features = pd.DataFrame()\n",
    "\n",
    "# Creating StanfordPOStagger. We will need it as a createFeatureVector function parameter\n",
    "jar='Stanford_POStagger/stanford-postagger.jar'\n",
    "model='Stanford_POStagger/models/english-bidirectional-distsim.tagger'\n",
    "st = StanfordPOSTagger(model,jar, encoding='utf-8')\n",
    "\n",
    "# Iterate over all the train entities (tuples of (sentence, drugs)) and apply the BIOTagger function\n",
    "for text,drugs in train_texts_entities:\n",
    "    features = pd.concat([features,createFeatureVector(text, drugbank_db,st)])\n",
    "    tuples = BOTagger(text, drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "# computing one-hot coding for 'Aa1-' feature.\n",
    "training_dummies = pd.get_dummies(features['Aa1-'])\n",
    "features = features.drop('Aa1-',axis=1)\n",
    "# joining both data frames\n",
    "for name in training_dummies.columns:\n",
    "    features[name]=training_dummies[name]\n",
    "\n",
    "'''\n",
    "# Remove these features columns that are all 0. We will find many in those indicating pos_tags\n",
    "for column in features.columns.values:\n",
    "    if sum(features[column])==0:\n",
    "        features = features.drop(column,axis=1)\n",
    "        removed_columns.append(column)\n",
    "'''\n",
    "\n",
    "# Create a training set with the features,tokens and the BIO tags\n",
    "train_df = features\n",
    "print(tokens)\n",
    "print(tags)\n",
    "train_df['token'] = tokens\n",
    "train_df['output'] = tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many tokens have we tagged with the DrugBank?\n",
    "print(sum(train_df['is_token_in_DrugBank_db'] == 1))\n",
    "\n",
    "# How many tokens are actually tagged with a 'B' or a 'I'?\n",
    "print(sum(train_df['output'] == 'B'))\n",
    "print(sum(train_df['output'] == 'I'))\n",
    "#train_df[train_df['is_token_in_DrugBank_db'] == 1]\n",
    "\n",
    "# Which are our unique values?\n",
    "print(train_df.output.unique())\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "X = train_df.loc[:, [all(x) for x in list(zip(train_df.columns!=target_name,train_df.columns!=token_name))]]\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning SVM in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a SVM object with the corresponding tunned parameters\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Look for the best parameters of the SVM model with GridSearchCV\n",
    "start = time.time()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "end = time.time()\n",
    "print('Execution time for GridSearchCV: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the SVM model with the parameters selected before\n",
    "start = time.time()\n",
    "clf.fit(X,Y)\n",
    "end = time.time()\n",
    "print('Training time of the SVM: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing training error. If there is a significant drop from training error to test error, we will be suffering \n",
    "# from overfitting\n",
    "\n",
    "train_predictions = []\n",
    "for text,entities in train_texts_entities:\n",
    "    # print('text: ', text)\n",
    "    # print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # computing predictions\n",
    "    features = createFeatureVector(text, drugbank_db,st)\n",
    "    \n",
    "    # computing one-hot coding for 'Aa1-' feature.\n",
    "    dummies = pd.get_dummies(features['Aa1-'])\n",
    "    features = features.drop('Aa1-',axis=1)\n",
    "    # joining both data frames\n",
    "    for name in dummies.columns:\n",
    "        features[name]=dummies[name]\n",
    "    \n",
    "    # adding those columns related to Aa1- that we cannot see with the sentence in question\n",
    "    for name in training_dummies.columns:\n",
    "        if name not in dummies.columns:\n",
    "            features[name]=[0]*len(dummies[dummies.columns.values[0]])\n",
    "    \n",
    "  \n",
    "    predicted_tags = clf.predict(features)\n",
    "    \n",
    "    \n",
    "    train_predictions.append((list(predicted_tags),entities,text)) \n",
    "    # print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    pred_entities = BOTagsToEntities(tokens = tokens, bo_tags = predicted_tags)\n",
    "    # print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "#print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "train_precision = []\n",
    "train_recall = []\n",
    "for tags, true_entities, text in train_predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = BOTagsToEntities(tokens,tags)\n",
    "    train_precision = train_precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    train_recall = train_recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(train_precision)\n",
    "avg_recall = statistics.mean(train_recall)\n",
    "print('train precision: ',avg_precision)\n",
    "print('train recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1_train = round((2*avg_precision*avg_recall) / (avg_precision + avg_recall),2)\n",
    "print('F1 train: ', F1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for text,entities in test_texts_entities:\n",
    "    print('original text: ', BOTagger(text = text, drugs = entities), '\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # computing predictions\n",
    "    features = createFeatureVector(text, drugbank_db,st)\n",
    "    \n",
    "    # computing one-hot coding for 'Aa1-' feature.\n",
    "    dummies = pd.get_dummies(features['Aa1-'])\n",
    "    features = features.drop('Aa1-',axis=1)\n",
    "    # joining both data frames\n",
    "    for name in dummies.columns:\n",
    "        features[name]=dummies[name]\n",
    "    \n",
    "    # adding those columns related to Aa1- that we cannot see with the sentence in question\n",
    "    for name in training_dummies.columns:\n",
    "        if name not in dummies.columns:\n",
    "            features[name]=[0]*len(dummies[dummies.columns.values[0]])\n",
    "            \n",
    "    '''\n",
    "    # removing those columns deleted when training the classifier\n",
    "    for column in features.columns.values:\n",
    "        if column in removed_columns:\n",
    "            features = features.drop(column,axis=1)\n",
    "    '''\n",
    "    \n",
    "    predicted_tags = clf.predict(features)\n",
    "    \n",
    "    predictions.append((list(predicted_tags),entities,text)) \n",
    "    print('predicted bio tags: ',str(list(zip(tokens, predicted_tags))),'\\n')\n",
    "    pred_entities = BOTagsToEntities(tokens = tokens, bo_tags = predicted_tags)\n",
    "    print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "    # TODO: Something is wrong with the BIOTagsToEntities()\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "#print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define a function that recover's the whole drug name from BIO taggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be based on $$F1=\\frac{2*precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover all the words from the predicted bio_tags and try to compute F1 for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision = []\n",
    "recall = []\n",
    "for tags, true_entities, text in predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = BOTagsToEntities(tokens,tags)\n",
    "    precision = precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    recall = recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "print('precision: ',avg_precision)\n",
    "print('recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1 = round((2*avg_precision*avg_recall) / (avg_precision + avg_recall),2)\n",
    "print('F1: ', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "## Log of results\n",
    "date, precision, recall, F1, features, test\n",
    "14-May, 46.2, 52.1, 48.99, Token length; Prefixes/Suffixes; POS tag; Binary features (+-2); Token position; DrugBank DB; Shape, yes\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
