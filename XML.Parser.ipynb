{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Load needed modules and specify the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "from lxml import etree # XML file parsing\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV # Parameter selection\n",
    "import time # Execution time of some blocks\n",
    "\n",
    "# Import our defined functions\n",
    "from drug_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the data directories\n",
    "train_dirs_whereto_parse = ['data/small_train_DrugBank']\n",
    "test_dirs_whereto_parse = ['data/small_test_DrugBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train and test data from the XML files\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the token 'STOP' at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('START Formal drug interaction studies have not been conducted with ORENCIA. STOP',\n",
       "  ['ORENCIA']),\n",
       " ('START Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance. STOP',\n",
       "  ['MTX', 'NSAIDs', 'corticosteroids', 'TNF blocking agents', 'abatacept'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRAINING DATA\n",
    "\n",
    "# Initialise the different lists with the data\n",
    "entities=[]\n",
    "texts=[]\n",
    "train_texts_entities = []\n",
    "\n",
    "# Iterate over all the different .xml files located in the specified directories\n",
    "for directory in train_dirs_whereto_parse:\n",
    "    \n",
    "    # Get the names of all the files in the directory and create a 'xml.root' object for\n",
    "    # each xml file\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in listdir(directory) if a.endswith('.xml')]\n",
    "    \n",
    "    # Iterate over all the different 'xml.root' objects to extract the needed information\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            train_texts_entities = train_texts_entities + [('START '+sentence.get('text')+' STOP', entities)]\n",
    "            entities =[]\n",
    "\n",
    "# train_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# Example: \n",
    "# [('I love Ibuprofeno and Frenadol', ['Ibuprofeno', 'Frenadol']), ('Give me a Fluimucil', ['Fluimucil'])]\n",
    "\n",
    "train_texts_entities[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\\n', ['testosterone', 'Plenaxis'])\n"
     ]
    }
   ],
   "source": [
    "## TESTING DATA\n",
    "\n",
    "# Same process as with the training data\n",
    "# In the testing data, for each sentance we have two related files:\n",
    "# - A file with a sentence to be parsed, in which we may encounter drug names (ending with 'text.txt')\n",
    "# - A file with the drug entities recognised in the sentence (ending with 'entities.txt')\n",
    "\n",
    "test_texts = []\n",
    "test_entities = []\n",
    "\n",
    "for directory in test_dirs_whereto_parse:\n",
    "    \n",
    "    # Si no poso el sorted, em llegeix els files amb un ordre aleatori.\n",
    "    # Amb el sorted m'asseguro que els corresponents files text.txt i entities.txt estan en la mateixa posicio\n",
    "    \n",
    "    # Read the pairs of files in alphabetical order\n",
    "    text_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('text.txt')])\n",
    "    entities_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('entities.txt')])\n",
    "    \n",
    "    for file in text_file_names:\n",
    "        file = open(file,'r')\n",
    "        test_texts = test_texts + [file.read()]\n",
    "        \n",
    "    for file in entities_file_names:\n",
    "        read_entities = []\n",
    "        with open(file,'r') as f:\n",
    "            for line in f:\n",
    "                read_entities = read_entities+[' '.join(line.split()[0:-1])] # separo en words, el.limino la ultima i torno a unir\n",
    "                \n",
    "        test_entities.append(read_entities)\n",
    "\n",
    "\n",
    "test_texts_entities=list(zip(test_texts,test_entities))\n",
    "\n",
    "\n",
    "# test_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there.\n",
    "print(test_texts_entities[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO Tagger\n",
    "\n",
    "In this section we will tag each sentence with the BIO format. For this, we have created a function called 'bioTagger' which will perform the following actions:\n",
    "\n",
    "Given a sentence 'text' and a set of drugs 'drugs', this function returns a list of str that\n",
    "contains a tag for each of the tokens in text. The tags can be either 'B', 'I' or 'O'. 'B' means\n",
    "the token is the first part of a drug entity, 'I' means the token is the continuation of a drug entity,\n",
    "and 'O' means that the token does not belong to a drug entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the needed lists\n",
    "tokens = []\n",
    "tags = []\n",
    "features = pd.DataFrame()\n",
    "# Iterate over all the train entities (tuples of (sentence, drugs)) and apply the bioTagger function\n",
    "for text,drugs in train_texts_entities:\n",
    "    features = pd.concat([features,pd.DataFrame(feature_vector(text))])\n",
    "    tuples = bioTagger(text, drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "    \n",
    "# Create a training set with the tokens and the BIO tags\n",
    "train_df = features\n",
    "train_df['token']=tokens\n",
    "train_df['output']=tags\n",
    "#train_set = {'token':tokens,'output':tags}\n",
    "#train_df = pd.DataFrame(train_set)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_digits</th>\n",
       "      <th>all_digits_context-1</th>\n",
       "      <th>all_digits_context-2</th>\n",
       "      <th>all_digits_context1</th>\n",
       "      <th>all_digits_context2</th>\n",
       "      <th>all_letters</th>\n",
       "      <th>all_letters_context-1</th>\n",
       "      <th>all_letters_context-2</th>\n",
       "      <th>all_letters_context1</th>\n",
       "      <th>all_letters_context2</th>\n",
       "      <th>...</th>\n",
       "      <th>initial_capital_letter</th>\n",
       "      <th>initial_capital_letter_context-1</th>\n",
       "      <th>initial_capital_letter_context-2</th>\n",
       "      <th>initial_capital_letter_context1</th>\n",
       "      <th>initial_capital_letter_context2</th>\n",
       "      <th>prefix_feature</th>\n",
       "      <th>suffix_feature</th>\n",
       "      <th>token_length</th>\n",
       "      <th>token</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>START</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Formal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>drug</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>interaction</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>studies</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   all_digits  all_digits_context-1  all_digits_context-2  \\\n",
       "0           0                     0                     0   \n",
       "1           0                     0                     0   \n",
       "2           0                     0                     0   \n",
       "3           0                     0                     0   \n",
       "4           0                     0                     0   \n",
       "\n",
       "   all_digits_context1  all_digits_context2  all_letters  \\\n",
       "0                    0                    0            1   \n",
       "1                    0                    0            0   \n",
       "2                    0                    0            0   \n",
       "3                    0                    0            0   \n",
       "4                    0                    0            0   \n",
       "\n",
       "   all_letters_context-1  all_letters_context-2  all_letters_context1  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     1   \n",
       "2                      1                      0                     1   \n",
       "3                      1                      1                     1   \n",
       "4                      1                      1                     1   \n",
       "\n",
       "   all_letters_context2   ...    initial_capital_letter  \\\n",
       "0                     0   ...                         1   \n",
       "1                     1   ...                         1   \n",
       "2                     1   ...                         0   \n",
       "3                     1   ...                         0   \n",
       "4                     1   ...                         0   \n",
       "\n",
       "   initial_capital_letter_context-1  initial_capital_letter_context-2  \\\n",
       "0                                 0                                 0   \n",
       "1                                 0                                 0   \n",
       "2                                 1                                 0   \n",
       "3                                 0                                 1   \n",
       "4                                 0                                 0   \n",
       "\n",
       "   initial_capital_letter_context1  initial_capital_letter_context2  \\\n",
       "0                                0                                0   \n",
       "1                                0                                0   \n",
       "2                                0                                0   \n",
       "3                                0                                0   \n",
       "4                                0                                0   \n",
       "\n",
       "   prefix_feature  suffix_feature  token_length        token  output  \n",
       "0               0               0             5        START       O  \n",
       "1               0               1             6       Formal       O  \n",
       "2               0               0             4         drug       O  \n",
       "3               0               0            11  interaction       O  \n",
       "4               0               0             7      studies       O  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "X = train_df.loc[:, [all(x) for x in list(zip(train_df.columns!=target_name,train_df.columns!=token_name))]]\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning SVM in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for GridSearchCV:  0.0002429485321044922\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM object with the corresponding tunned parameters\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Look for the best parameters of the SVM model with GridSearchCV\n",
    "start = time.time()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "end = time.time()\n",
    "print('Execution time for GridSearchCV: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time of the SVM:  2.0113768577575684\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model with the parameters selected before\n",
    "start = time.time()\n",
    "clf.fit(X,Y)\n",
    "end = time.time()\n",
    "print('Training time of the SVM: ', str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with just one test text. Let's tokenize it, and create its feature vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  No drug, nutritional supplement, food or herb interactions have yet been reported.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  [] \n",
      "\n",
      "text:  No formal drug/drug interaction studies with Plenaxis were performed.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis'] \n",
      "\n",
      "text:  Cytochrome P-450 is not known to be involved in the metabolism of Plenaxis.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  [] \n",
      "\n",
      "text:  Plenaxis is highly bound to plasma proteins (96 to 99%).\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['proteins'] \n",
      "\n",
      "text:  Laboratory Tests Response to Plenaxis should be monitored by measuring serum total testosterone concentrations just prior to administration on Day 29 and every 8 weeks thereafter.\n",
      "\n",
      "real entities:  ['testosterone', 'Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'B' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis', 'total', 'testosterone'] \n",
      "\n",
      "text:  Serum transaminase levels should be obtained before starting treatment with Plenaxis and periodically during treatment.\n",
      "\n",
      "real entities:  ['Plenaxis'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Plenaxis'] \n",
      "\n",
      "text:  Periodic measurement of serum PSA levels may also be considered.\n",
      "\n",
      "real entities:  [] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['PSA'] \n",
      "\n",
      "text:  Formal drug interaction studies have not been conducted with ORENCIA.\n",
      "\n",
      "real entities:  ['ORENCIA'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['Formal'] \n",
      "\n",
      "text:  Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance.\n",
      "\n",
      "real entities:  ['abatacept', 'corticosteroids', 'TNF blocking agents', 'MTX', 'NSAIDs'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['MTX', 'NSAIDs', 'corticosteroids'] \n",
      "\n",
      "text:  The majority of patients in RA clinical studies received one or more of the following concomitant medications with ORENCIA: MTX, NSAIDs, corticosteroids, TNF blocking agents, azathioprine, chloroquine, gold, hydroxychloroquine, leflunomide, sulfasalazine, and anakinra.\n",
      "\n",
      "real entities:  ['anakinra', 'sulfasalazine', 'corticosteroids', 'NSAIDs', 'MTX', 'ORENCIA', 'gold', 'chloroquine', 'azathioprine', 'TNF blocking agents', 'leflunomide', 'hydroxychloroquine'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O' 'B' 'O'\n",
      " 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['RA', 'one', 'ORENCIA', 'MTX', 'NSAIDs', 'corticosteroids', 'TNF', 'agents', 'azathioprine', 'chloroquine', 'gold', 'hydroxychloroquine', 'leflunomide', 'sulfasalazine'] \n",
      "\n",
      "text:  Concurrent administration of a TNF antagonist with ORENCIA has been associated with an increased risk of serious infections and no significant additional efficacy over use of the TNF antagonists alone.\n",
      "\n",
      "real entities:  ['TNF antagonist', 'ORENCIA', 'TNF antagonists'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O'] \n",
      "\n",
      "predicted entities:  ['TNF', 'ORENCIA', 'additional', 'TNF'] \n",
      "\n",
      "text:  Concurrent therapy with ORENCIA and TNF antagonists is not recommended.\n",
      "\n",
      "real entities:  ['TNF antagonists', 'ORENCIA'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA', 'TNF'] \n",
      "\n",
      "text:  There is insufficient experience to assess the safety and efficacy of ORENCIA administered concurrently with anakinra, and therefore such use is not recommended.\n",
      "\n",
      "real entities:  ['ORENCIA', 'anakinra'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'B' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['ORENCIA', 'anakinra'] \n",
      "\n",
      "text:  Formal drug interaction studies with Abciximab have not been conducted.\n",
      "\n",
      "real entities:  ['Abciximab'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['Formal', 'Abciximab'] \n",
      "\n",
      "text:  Abciximab has been administered to patients with ischemic heart disease treated concomitantly with a broad range of medications used in the treatment of angina myocardial infarction and hypertension.\n",
      "\n",
      "real entities:  ['Abciximab'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['myocardial'] \n",
      "\n",
      "text:  These medications have included heparin, warfarin, beta-adrenergic receptor blockers, calcium channel antagonists, angiotensin converting enzyme inhibitors, intravenous and oral nitrates, ticlopidine, and aspirin.\n",
      "\n",
      "real entities:  ['aspirin', 'ticlopidine', 'nitrates', 'angiotensin converting enzyme inhibitors', 'calcium channel antagonists', 'beta-adrenergic receptor blockers', 'warfarin', 'heparin'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'I' 'B' 'O' 'B' 'O' 'B' 'O' 'O' 'O'\n",
      " 'O' 'B' 'O' 'O' 'O' 'O' 'B' 'O' 'B' 'O' 'O' 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['heparin', 'warfarin', 'blockers', 'calcium', 'antagonists', 'inhibitors', 'nitrates', 'ticlopidine'] \n",
      "\n",
      "text:  Heparin, other anticoagulants, thrombolytics, and anti platelet agents are associated with an increase in bleeding.\n",
      "\n",
      "real entities:  ['Heparin', 'anticoagulants', 'thrombolytics', 'anti platelet agents'] \n",
      "\n",
      "predicted bio tags:  ['B' 'O' 'O' 'O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O'] \n",
      "\n",
      "predicted entities:  ['Heparin', 'thrombolytics'] \n",
      "\n",
      "text:  Patients with HACA titers may have allergic or hypersensitivity reactions when treated with other diagnostic or therapeutic monoclonal antibodies.\n",
      "\n",
      "real entities:  ['therapeutic monoclonal antibodies', 'diagnostic monoclonal antibodies'] \n",
      "\n",
      "predicted bio tags:  ['O' 'O' 'B' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n",
      " 'O' 'O'] \n",
      "\n",
      "predicted entities:  ['HACA'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesc/Dropbox/MIRI-Data Science/4Q/AHLT/Project/drug-interactions/drug_functions.py:196: UserWarning: One of the tags was not recognised. Please check the \"bio_tags\" parameter.\n",
      "  warnings.warn('One of the tags was not recognised. Please check the \"bio_tags\" parameter.')\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text,entities in test_texts_entities:\n",
    "    print('text: ', text)\n",
    "    print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_tags = clf.predict(pd.DataFrame(feature_vector(text)))\n",
    "    predictions.append((list(predicted_tags),entities,text)) \n",
    "    print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    pred_entities = bioTagsToEntities(tokens = tokens, bio_tags = predicted_tags)\n",
    "    print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "# print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define a function that recover's the whole drug name from BIO taggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be based on $$F1=\\frac{2*precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest exemple m'ha ajudat a entendre com calcular la precision i la recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "true = ['hola','que','ca','bo']\n",
    "pred = ['hola','que','pet']\n",
    "\n",
    "# Precision\n",
    "print(round(len([word for word in pred if word in true])/len(pred),2))\n",
    "\n",
    "# Recall\n",
    "print(round(len([word for word in pred if word in true])/len(true),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_precision(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(pred_ent),2)*100     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_recall(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(true_ent),2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover all the words from the predicted bio_tags and try to compute F1 for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  45.166666666666664\n",
      "recall:  45.888888888888886\n",
      "F1:  45.52491356518202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesc/Dropbox/MIRI-Data Science/4Q/AHLT/Project/drug-interactions/drug_functions.py:196: UserWarning: One of the tags was not recognised. Please check the \"bio_tags\" parameter.\n",
      "  warnings.warn('One of the tags was not recognised. Please check the \"bio_tags\" parameter.')\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "precision = []\n",
    "recall = []\n",
    "for tags, true_entities, text in predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = bioTagsToEntities(tokens,tags)\n",
    "    precision = precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    recall = recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "print('precision: ',avg_precision)\n",
    "print('recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1 = (2*avg_precision*avg_recall) / (avg_precision + avg_recall)\n",
    "print('F1: ', F1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
