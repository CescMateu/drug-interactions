{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Load needed modules and specify the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "from lxml import etree # XML file parsing\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV # Parameter selection\n",
    "import time # Execution time of some blocks\n",
    "\n",
    "# Import our defined functions\n",
    "from drug_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directories\n",
    "train_dirs_whereto_parse = ['data/small_train_DrugBank']\n",
    "test_dirs_whereto_parse = ['data/small_test_DrugBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train and test data from the XML files\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the token 'STOP' at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING DATA\n",
    "\n",
    "# Initialise the different lists with the data\n",
    "entities=[]\n",
    "texts=[]\n",
    "train_texts_entities = []\n",
    "\n",
    "# Iterate over all the different .xml files located in the specified directories\n",
    "for directory in train_dirs_whereto_parse:\n",
    "    \n",
    "    # Get the names of all the files in the directory and create a 'xml.root' object for\n",
    "    # each xml file\n",
    "    roots = [etree.parse(directory+'/'+a).getroot() for a in listdir(directory) if a.endswith('.xml')]\n",
    "    \n",
    "    # Iterate over all the different 'xml.root' objects to extract the needed information\n",
    "    for root in roots:\n",
    "        for sentence in root.findall('sentence'):\n",
    "            for entity in sentence.findall('entity'):\n",
    "                entities = entities+[entity.get('text')]\n",
    "            train_texts_entities = train_texts_entities + [('START '+sentence.get('text')+' STOP', entities)]\n",
    "            entities =[]\n",
    "\n",
    "# train_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there\n",
    "# Example: \n",
    "# [('I love Ibuprofeno and Frenadol', ['Ibuprofeno', 'Frenadol']), ('Give me a Fluimucil', ['Fluimucil'])]\n",
    "\n",
    "train_texts_entities[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING DATA\n",
    "\n",
    "# Same process as with the training data\n",
    "# In the testing data, for each sentance we have two related files:\n",
    "# - A file with a sentence to be parsed, in which we may encounter drug names (ending with 'text.txt')\n",
    "# - A file with the drug entities recognised in the sentence (ending with 'entities.txt')\n",
    "\n",
    "test_texts = []\n",
    "test_entities = []\n",
    "\n",
    "for directory in test_dirs_whereto_parse:\n",
    "    \n",
    "    # Si no poso el sorted, em llegeix els files amb un ordre aleatori.\n",
    "    # Amb el sorted m'asseguro que els corresponents files text.txt i entities.txt estan en la mateixa posicio\n",
    "    \n",
    "    # Read the pairs of files in alphabetical order\n",
    "    text_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('text.txt')])\n",
    "    entities_file_names = sorted([directory + '/' + file for file in listdir(directory) if file.endswith('entities.txt')])\n",
    "    \n",
    "    for file in text_file_names:\n",
    "        file = open(file,'r')\n",
    "        test_texts = test_texts + [file.read()]\n",
    "        \n",
    "    for file in entities_file_names:\n",
    "        read_entities = []\n",
    "        with open(file,'r') as f:\n",
    "            for line in f:\n",
    "                read_entities = read_entities+[' '.join(line.split()[0:-1])] # separo en words, el.limino la ultima i torno a unir\n",
    "                \n",
    "        test_entities.append(read_entities)\n",
    "\n",
    "\n",
    "test_texts_entities=list(zip(test_texts,test_entities))\n",
    "\n",
    "\n",
    "# test_texts_entities is a list of tuples. Each one is comprised of the sentence and the drugs in there.\n",
    "print(test_texts_entities[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO Tagger\n",
    "\n",
    "In this section we will tag each sentence with the BIO format. For this, we have created a function called 'bioTagger' which will perform the following actions:\n",
    "\n",
    "Given a sentence 'text' and a set of drugs 'drugs', this function returns a list of str that\n",
    "contains a tag for each of the tokens in text. The tags can be either 'B', 'I' or 'O'. 'B' means\n",
    "the token is the first part of a drug entity, 'I' means the token is the continuation of a drug entity,\n",
    "and 'O' means that the token does not belong to a drug entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the needed lists\n",
    "tokens = []\n",
    "tags = []\n",
    "removed_columns = []\n",
    "features = pd.DataFrame()\n",
    "# Iterate over all the train entities (tuples of (sentence, drugs)) and apply the bioTagger function\n",
    "for text,drugs in train_texts_entities:\n",
    "    features = pd.concat([features,createFeatureVector(text)])\n",
    "    tuples = bioTagger(text, drugs)\n",
    "    tokens = tokens + [word[0] for word in tuples]\n",
    "    tags = tags + [word[1] for word in tuples]\n",
    "\n",
    "# removing these features columns that are all 0. We will find many in those indicating pos_tags\n",
    "for column in features.columns.values:\n",
    "    if sum(features[column])==0:\n",
    "        features = features.drop(column,axis=1)\n",
    "        removed_columns.append(column)\n",
    "\n",
    "# Create a training set with the features,tokens and the BIO tags\n",
    "train_df = features\n",
    "train_df['token']=tokens\n",
    "train_df['output']=tags\n",
    "#train_set = {'token':tokens,'output':tags}\n",
    "#train_df = pd.DataFrame(train_set)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the classifier\n",
    "## Support Vector Machines\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the target variable\n",
    "target_name = 'output'\n",
    "token_name = 'token'\n",
    "\n",
    "# Create the appropiate data structure to pass it to the SVM.\n",
    "# X columns should be all but target_name and token_name\n",
    "X = train_df.loc[:, [all(x) for x in list(zip(train_df.columns!=target_name,train_df.columns!=token_name))]]\n",
    "Y = train_df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning SVM in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM object with the corresponding tunned parameters\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Look for the best parameters of the SVM model with GridSearchCV\n",
    "start = time.time()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "end = time.time()\n",
    "print('Execution time for GridSearchCV: ', str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM model with the parameters selected before\n",
    "start = time.time()\n",
    "clf.fit(X,Y)\n",
    "end = time.time()\n",
    "print('Training time of the SVM: ', str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with just one test text. Let's tokenize it, and create its feature vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for text,entities in test_texts_entities:\n",
    "    #print('text: ', text)\n",
    "    #print('real entities: ',entities,'\\n')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # computing predictions\n",
    "    features = createFeatureVector(text)\n",
    "    \n",
    "    # removing those columns deleted when training the classifier\n",
    "    for column in features.columns.values:\n",
    "        if column in removed_columns:\n",
    "            features = features.drop(column,axis=1)\n",
    "    predicted_tags = clf.predict(features)\n",
    "    \n",
    "    predictions.append((list(predicted_tags),entities,text)) \n",
    "    #print('predicted bio tags: ',predicted_tags,'\\n')\n",
    "    pred_entities = bioTagsToEntities(tokens = tokens, bio_tags = predicted_tags)\n",
    "    #print('predicted entities: ', pred_entities, '\\n')\n",
    "    \n",
    "# predictions is a list of tupples comprised of predicted tags and the true drugs we should extract from there\n",
    "# print('predictions of text 1: ',predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define a function that recover's the whole drug name from BIO taggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be based on $$F1=\\frac{2*precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(pred_ent),2)*100     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(pred_ent,true_ent):\n",
    "    if len(pred_ent) == 0 or len(true_ent) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(len([word for word in pred_ent if word in true_ent])/len(true_ent),2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover all the words from the predicted bio_tags and try to compute F1 for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "precision = []\n",
    "recall = []\n",
    "for tags, true_entities, text in predictions:\n",
    "    # I need the tokens for the bioTagsToEntities function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    predicted_entities = bioTagsToEntities(tokens,tags)\n",
    "    precision = precision + [compute_precision(predicted_entities,true_entities)]\n",
    "    recall = recall + [compute_recall(predicted_entities,true_entities)]\n",
    "\n",
    "    \n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "print('precision: ',avg_precision)\n",
    "print('recall: ',avg_recall)\n",
    "\n",
    "# F1 metric\n",
    "F1 = round((2*avg_precision*avg_recall) / (avg_precision + avg_recall),2)\n",
    "print('F1: ', F1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
