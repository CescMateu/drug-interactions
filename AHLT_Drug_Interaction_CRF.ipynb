{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split # Parameter selection\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Other libraries\n",
    "import time # Execution time of some blocks\n",
    "import statistics\n",
    "from IPython.display import display # For displaying DataFrames correctly in Jupyter\n",
    "from itertools import chain\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# Import our own defined functions\n",
    "from xlm_parsers_functions import *\n",
    "from drug_interaction_functions import *\n",
    "from drug_functions import *\n",
    "from NER_functions import *\n",
    "from ortographic_features import *\n",
    "from context_features import *\n",
    "from feature_creation_interaction import *\n",
    "from crf_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of this part\n",
    "In this second part of the project, we will focus on two different things: \n",
    "1. Detection of interactions between drugs\n",
    "2. Classification of each drug-drug interaction according to one of the following types:\n",
    "    - Advice: 'Interactions may be expected, and Uroxatral should not be used in combination with other alpha-blockers.'\n",
    "    - Effect: 'In uninfected volunteers, 46% developed rash while receiving Sustiva and Clarithromycin.'\n",
    "    - Mechanism: 'Grepafloxacin is a competitive inhibitor of the metabolism of theophylline'.\n",
    "    - Int: 'The interaction of omeprazole and ketoconazole has been stablished.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the XML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n\\n# Training data\\n\\ntrain_data_dir_DrugBank = 'data/Train/DrugBank/'\\ntrain_data_dir_MedLine = 'data/Train/MedLine/'\\ntrain_data_dirs = [train_data_dir_DrugBank, train_data_dir_MedLine]\\n\\n\\nXMLdata_train = []\\nfor train_data_dir in train_data_dirs:\\n    XMLdata_train = XMLdata_train + readXMLData(train_data_dir)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# Training data\n",
    "\n",
    "train_data_dir_DrugBank = 'data/Train/DrugBank/'\n",
    "train_data_dir_MedLine = 'data/Train/MedLine/'\n",
    "train_data_dirs = [train_data_dir_DrugBank, train_data_dir_MedLine]\n",
    "\n",
    "\n",
    "XMLdata_train = []\n",
    "for train_data_dir in train_data_dirs:\n",
    "    XMLdata_train = XMLdata_train + readXMLData(train_data_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n\\n# Testing data\\n\\ntest_data_dir_DrugBank = 'data/Test/Test_data_DDI/DrugBank/'\\ntest_data_dir_MedLine = 'data/Test/Test_data_DDI/MedLine/'\\ntest_data_dirs = [test_data_dir_DrugBank, test_data_dir_MedLine]\\n\\nXMLdata_test = []\\nfor test_data_dir in test_data_dirs:\\n    XMLdata_test = XMLdata_test + readXMLData(test_data_dir)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# Testing data\n",
    "\n",
    "test_data_dir_DrugBank = 'data/Test/Test_data_DDI/DrugBank/'\n",
    "test_data_dir_MedLine = 'data/Test/Test_data_DDI/MedLine/'\n",
    "test_data_dirs = [test_data_dir_DrugBank, test_data_dir_MedLine]\n",
    "\n",
    "XMLdata_test = []\n",
    "for test_data_dir in test_data_dirs:\n",
    "    XMLdata_test = XMLdata_test + readXMLData(test_data_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the parsed XML data for later use\n",
    "As we don't want to read and parse the XML files each time, we will save them into a pickle object that we can quickly read when needed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"parsed_train.txt\", \"wb\") as f:   #Pickling\\n    pickle.dump(XMLdata_train, f)\\n\\nwith open(\"parsed_test.txt\", \"wb\") as f:   #Pickling\\n    pickle.dump(XMLdata_test, f)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open(\"parsed_train.txt\", \"wb\") as f:   #Pickling\n",
    "    pickle.dump(XMLdata_train, f)\n",
    "\n",
    "with open(\"parsed_test.txt\", \"wb\") as f:   #Pickling\n",
    "    pickle.dump(XMLdata_test, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the saved parsed XML data quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"parsed_train.txt\", \"rb\") as f:   # Unpickling\\n    XMLdata_train = pickle.load(f)\\n    \\nwith open(\"parsed_test.txt\", \"rb\") as f:   # Unpickling\\n    XMLdata_test = pickle.load(f)\\n\\nprint(\\'Number of training sentences readed: \\', len(XMLdata_train))\\nprint(\\'Number of testing sentences readed: \\', len(XMLdata_test))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open(\"parsed_train.txt\", \"rb\") as f:   # Unpickling\n",
    "    XMLdata_train = pickle.load(f)\n",
    "    \n",
    "with open(\"parsed_test.txt\", \"rb\") as f:   # Unpickling\n",
    "    XMLdata_test = pickle.load(f)\n",
    "\n",
    "print('Number of training sentences readed: ', len(XMLdata_train))\n",
    "print('Number of testing sentences readed: ', len(XMLdata_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create FrequencyDistribution\n",
    "\n",
    "We will need it to build a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlist_of_list_of_tokens=[row[5] for row in XMLdata_train]\\nlist_of_tokens = sum(list_of_list_of_tokens,[])\\nfrequencyDistribution = nltk.FreqDist(list_of_tokens)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "list_of_list_of_tokens=[row[5] for row in XMLdata_train]\n",
    "list_of_tokens = sum(list_of_list_of_tokens,[])\n",
    "frequencyDistribution = nltk.FreqDist(list_of_tokens)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of features\n",
    "Before training our model, we need to come up with features to help us determine whether there is a relationship between the two drugs or not.\n",
    "\n",
    "Some ideas for features are the following:\n",
    "- Does the sentence contain a modal verb (should, must,...) between the two entities?\n",
    "- Word bigrams: This is a binary feature for all word bigrams that appeared more than once in the corpus, indicating the presence or absence of each such bigram in the sentence\n",
    "- Number of words between a pair of drugs\n",
    "- Number of drugs between a pair of drugs\n",
    "- POS of words between a pair of drugs: This is a binary feature for word POS tags obtained from POS tagging, and indicates the presence or absence of each POS between the two main drugs.\n",
    "- Path between a pair of drugs: Path between two main drugs in the parse tree is another feature in our system. Because syntactic paths are in general a sparse feature, we reduced the sparsity by collapsing identical adjacent non-terminal labels. E.g., NP-S-VP-VP-NP is converted to NP-S-VP-NP. This technique decreased the number of paths by 24.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n# Read the database\\nwith(open('data/DrugBank_names_DB.txt', 'r')) as f:\\n    drugbank_db = f.read().splitlines()\\n\\n# Create the train and tests datasets\\n\\n# Train\\nX_train = [[text2features(s, drugbank_db,frequencyDistribution)] for s in XMLdata_train]\\ny_train_int = [[text2interaction(s)] for s in XMLdata_train]\\ny_train_type = [[text2interactionType(s)] for s in XMLdata_train]\\n\\n# Test\\nX_test = [[text2features(s, drugbank_db,frequencyDistribution)] for s in XMLdata_test]\\ny_test_int = [[text2interaction(s)] for s in XMLdata_test]\\ny_test_type = [[text2interactionType(s)] for s in XMLdata_test]\\n\\nprint('Number of training sentences: ', len(X_train))\\nprint('Number of testing sentences: ', len(X_test))\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "# Read the database\n",
    "with(open('data/DrugBank_names_DB.txt', 'r')) as f:\n",
    "    drugbank_db = f.read().splitlines()\n",
    "\n",
    "# Create the train and tests datasets\n",
    "\n",
    "# Train\n",
    "X_train = [[text2features(s, drugbank_db,frequencyDistribution)] for s in XMLdata_train]\n",
    "y_train_int = [[text2interaction(s)] for s in XMLdata_train]\n",
    "y_train_type = [[text2interactionType(s)] for s in XMLdata_train]\n",
    "\n",
    "# Test\n",
    "X_test = [[text2features(s, drugbank_db,frequencyDistribution)] for s in XMLdata_test]\n",
    "y_test_int = [[text2interaction(s)] for s in XMLdata_test]\n",
    "y_test_type = [[text2interactionType(s)] for s in XMLdata_test]\n",
    "\n",
    "print('Number of training sentences: ', len(X_train))\n",
    "print('Number of testing sentences: ', len(X_test))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time\\n\\nwith open(\"X_train.txt\", \"wb\") as f:   # Unpickling\\n    pickle.dump(X_train, f)\\n    \\nwith open(\"y_train_int.txt\", \"wb\") as f:   # Unpickling\\n    pickle.dump(y_train_int, f)\\n    \\nwith open(\"y_train_type.txt\", \"wb\") as f:   # Unpickling\\n    pickle.dump(y_train_type, f)\\n\\nwith open(\"X_test.txt\", \"wb\") as f:   # Unpickling\\n    pickle.dump(X_test, f)\\n    \\nwith open(\"y_test_int.txt\", \"wb\") as f:   # Unpickling\\n    pickle.dump(y_test_int, f)\\n    \\nwith open(\"y_test_type.txt\", \"wb\") as f:   # Unpickling\\n    pickle.dump(y_test_type, f)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "with open(\"X_train.txt\", \"wb\") as f:   # Unpickling\n",
    "    pickle.dump(X_train, f)\n",
    "    \n",
    "with open(\"y_train_int.txt\", \"wb\") as f:   # Unpickling\n",
    "    pickle.dump(y_train_int, f)\n",
    "    \n",
    "with open(\"y_train_type.txt\", \"wb\") as f:   # Unpickling\n",
    "    pickle.dump(y_train_type, f)\n",
    "\n",
    "with open(\"X_test.txt\", \"wb\") as f:   # Unpickling\n",
    "    pickle.dump(X_test, f)\n",
    "    \n",
    "with open(\"y_test_int.txt\", \"wb\") as f:   # Unpickling\n",
    "    pickle.dump(y_test_int, f)\n",
    "    \n",
    "with open(\"y_test_type.txt\", \"wb\") as f:   # Unpickling\n",
    "    pickle.dump(y_test_type, f)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences readed:  27792\n",
      "Number of testing sentences readed:  5716\n",
      "CPU times: user 2.08 s, sys: 351 ms, total: 2.43 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "with open(\"X_train.txt\", \"rb\") as f:   # Unpickling\n",
    "    X_train = pickle.load(f)\n",
    "    \n",
    "with open(\"y_train_int.txt\", \"rb\") as f:   # Unpickling\n",
    "    y_train_int = pickle.load(f)\n",
    "    \n",
    "with open(\"y_train_type.txt\", \"rb\") as f:   # Unpickling\n",
    "    y_train_type = pickle.load(f)\n",
    "\n",
    "with open(\"X_test.txt\", \"rb\") as f:   # Unpickling\n",
    "    X_test = pickle.load(f)\n",
    "    \n",
    "with open(\"y_test_int.txt\", \"rb\") as f:   # Unpickling\n",
    "    y_test_int = pickle.load(f)\n",
    "    \n",
    "with open(\"y_test_type.txt\", \"rb\") as f:   # Unpickling\n",
    "    y_test_type = pickle.load(f)\n",
    "\n",
    "print('Number of training sentences readed: ', len(X_train))\n",
    "print('Number of testing sentences readed: ', len(X_test))\n",
    "''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Classifying interactions between true/false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Using training data and the parameters obtained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 37.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization took 2257.54 seconds to complete\n",
      "best params: {'c1': 1.0458554701644089, 'c2': 0.006237772066168241}\n",
      "best CV score: 0.867758006638595\n",
      "model size: 0.66M\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       true      0.524     0.531     0.527       979\n",
      "      false      0.903     0.900     0.901      4737\n",
      "\n",
      "avg / total      0.838     0.837     0.837      5716\n",
      "\n",
      "CPU times: user 10min 15s, sys: 54.1 s, total: 11min 9s\n",
      "Wall time: 38min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mod1 = trainCRFAndEvaluate(\n",
    "            X_train = X_train, \n",
    "            y_train = y_train_int,\n",
    "            X_test = X_test,\n",
    "            y_test = y_test_int,\n",
    "            labels = ['true', 'false'],\n",
    "            hyperparam_optim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_int = mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Classifying between types of interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences:  4021\n",
      "Number of testing sentences:  993\n"
     ]
    }
   ],
   "source": [
    "X_train_filtered = []\n",
    "y_train_filtered = []\n",
    "for idx,val in enumerate(y_train_int):\n",
    "    if val==['true']:\n",
    "        X_train_filtered.append(X_train[idx])\n",
    "        y_train_filtered.append(y_train_type[idx]) # no we don't want true or false rather than int, mechanism...\n",
    "\n",
    "\n",
    "X_test_filtered = []\n",
    "y_test_filtered = []\n",
    "for idx,val in enumerate(y_pred_int):\n",
    "    if val==['true']:\n",
    "        X_test_filtered.append(X_test[idx])\n",
    "        y_test_filtered.append(y_test_type[idx]) # no we don't want true or false rather than int, mechanism...\n",
    "\n",
    "print('Number of training sentences: ', len(X_train_filtered))\n",
    "print('Number of testing sentences: ', len(X_test_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization took 384.2 seconds to complete\n",
      "best params: {'c1': 0.017187836718689203, 'c2': 0.0056279023357100012}\n",
      "best CV score: 0.7362345683497273\n",
      "model size: 0.88M\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  mechanism      0.332     0.874     0.482       135\n",
      "     advise      0.421     0.726     0.533       124\n",
      "     effect      0.423     0.866     0.569       194\n",
      "        int      0.556     0.224     0.319        67\n",
      "\n",
      "avg / total      0.416     0.752     0.505       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod2 = trainCRFAndEvaluate(\n",
    "            X_train = X_train_filtered, \n",
    "            y_train = y_train_filtered,\n",
    "            X_test = X_test_filtered,\n",
    "            y_test = y_test_filtered,\n",
    "            labels = ['mechanism', 'advise', 'effect', 'int'],\n",
    "            hyperparam_optim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  mechanism      0.332     0.391     0.359       302\n",
      "     advise      0.421     0.407     0.414       221\n",
      "     effect      0.423     0.467     0.444       360\n",
      "        int      0.556     0.156     0.244        96\n",
      "\n",
      "avg / total      0.408     0.399     0.391       979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_type = mod2.predict(X_test_filtered)\n",
    "\n",
    "# Join the obtained predictions with the predictions made in the first model\n",
    "joinResultsFirstSecondModel(y_test_type = y_test_type, y_pred_type = y_pred_type, y_pred_int = y_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n%%time\\n# Define fixed parameters and parameters to search\\ncrf = sklearn_crfsuite.CRF(\\n    algorithm='lbfgs',\\n    max_iterations=100,\\n    all_possible_transitions=True\\n)\\n\\n# Parameter search\\n\\n# use the same metric for evaluation\\nf1_scorer = make_scorer(metrics.flat_f1_score,\\n                        average='weighted')\\n\\nparams_space = {\\n    'c1': scipy.stats.expon(scale=0.5),\\n    'c2': scipy.stats.expon(scale=0.05),\\n}\\n\\nrs = RandomizedSearchCV(crf, params_space,\\n                        cv=3,\\n                        verbose=1,\\n                        n_jobs=-1,\\n                        n_iter=1,\\n                        scoring=f1_scorer)\\n\\nrs.fit(X_train, y_int_train)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "%%time\n",
    "# Define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# Parameter search\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted')\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=1,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(X_train, y_int_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# crf = rs.best_estimator_\\nprint('best params:', rs.best_params_)\\nprint('best CV score:', rs.best_score_)\\nprint('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_x = [s.parameters[\\'c1\\'] for s in rs.grid_scores_]\\n_y = [s.parameters[\\'c2\\'] for s in rs.grid_scores_]\\n_c = [s.mean_validation_score for s in rs.grid_scores_]\\n\\nfig = plt.figure()\\nfig.set_size_inches(12, 12)\\nax = plt.gca()\\nax.set_yscale(\\'log\\')\\nax.set_xscale(\\'log\\')\\nax.set_xlabel(\\'C1\\')\\nax.set_ylabel(\\'C2\\')\\nax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\\n    min(_c), max(_c)\\n))\\n\\nax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\\n\\nprint(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef print_state_features(state_features):\\n    for (attr, label), weight in state_features:\\n        print(\"%0.6f %-8s %s\" % (weight, label, attr))\\n\\nprint(\"Top positive:\")\\nprint_state_features(collections.Counter(crf.state_features_).most_common(15))\\n\\nprint(\"\\nTop negative:\")\\nprint_state_features(collections.Counter(crf.state_features_).most_common()[-15:])\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(collections.Counter(crf.state_features_).most_common(15))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(collections.Counter(crf.state_features_).most_common()[-15:])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
