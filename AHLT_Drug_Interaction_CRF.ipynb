{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split # Parameter selection\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Other libraries\n",
    "import time # Execution time of some blocks\n",
    "import statistics\n",
    "from IPython.display import display # For displaying DataFrames correctly in Jupyter\n",
    "from itertools import chain\n",
    "import collections\n",
    "\n",
    "# Import our own defined functions\n",
    "from xlm_parsers_functions import *\n",
    "from drug_interaction_functions import *\n",
    "from drug_functions import *\n",
    "from NER_functions import *\n",
    "from ortographic_features import *\n",
    "from context_features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of this part\n",
    "In this second part of the project, we will focus on two different things: \n",
    "1. Detection of interactions between drugs\n",
    "2. Classification of each drug-drug interaction according to one of the following types:\n",
    "    - Advice: 'Interactions may be expected, and Uroxatral should not be used in combination with other alpha-blockers.'\n",
    "    - Effect: 'In uninfected volunteers, 46% developed rash while receiving Sustiva and Clarithromycin.'\n",
    "    - Mechanism: 'Grepafloxacin is a competitive inhibitor of the metabolism of theophylline'.\n",
    "    - Int: The interaction of omeprazole and ketoconazole has been stablished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the XML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 926 ms, sys: 40.1 ms, total: 966 ms\n",
      "Wall time: 967 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "data_dir1 = 'data/Train/DrugBank/'\n",
    "data_dir2 = 'data/Train/MedLine/'\n",
    "'''\n",
    "\n",
    "data_dir1 = 'data/small_train_DrugBank/'\n",
    "# TODO: Read test data from the correct files\n",
    "\n",
    "def readXMLData(data_dir):\n",
    "\n",
    "    # Use xlm_element.tag to get the name of the xlm element\n",
    "    # Use xlm_element.attrib to get all the attributes of the xlm element as a string\n",
    "\n",
    "    # Parse the DrugBank Files\n",
    "    drugs_dataset = []\n",
    "    #parent_directory = '../LaboCase/small_train_DrugBank/'\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            # Parse the file\n",
    "            tree = ET.parse(data_dir + filename)\n",
    "            # Create a list of lists with the interactions of the file\n",
    "            drugs_dataset = drugs_dataset + listDDIFromXML(tree.getroot())\n",
    "\n",
    "    return(drugs_dataset)\n",
    "\n",
    "# Create a list of lists with the interactions of the file\n",
    "XMLdata = readXMLData(data_dir1)\n",
    "#XMLdata_MedLine = readXMLData(data_dir2)\n",
    "\n",
    "#XMLdata = XMLdata_DrugBank #+ XMLdata_MedLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total sentences in the training dataset: 308\n"
     ]
    }
   ],
   "source": [
    "print('Number of total sentences in the training dataset:', len(XMLdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Population pharmacokinetic analyses revealed that MTX, NSAIDs, corticosteroids, and TNF blocking agents did not influence abatacept clearance.',\n",
       " 'MTX',\n",
       " 'corticosteroids',\n",
       " ['MTX', 'NSAIDs', 'corticosteroids', 'TNF blocking agents', 'abatacept'],\n",
       " ['NNP',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'VBD',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  ',',\n",
       "  'NNP',\n",
       "  ',',\n",
       "  'NNS',\n",
       "  ',',\n",
       "  'CC',\n",
       "  'NNP',\n",
       "  'VBG',\n",
       "  'NNS',\n",
       "  'VBD',\n",
       "  'RB',\n",
       "  'VB',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  '.'],\n",
       " ['Population',\n",
       "  'pharmacokinetic',\n",
       "  'analyses',\n",
       "  'revealed',\n",
       "  'that',\n",
       "  'MTX',\n",
       "  'NSAIDs',\n",
       "  ',',\n",
       "  'corticosteroids',\n",
       "  ',',\n",
       "  'and',\n",
       "  'TNF',\n",
       "  'blocking',\n",
       "  'agents',\n",
       "  'did',\n",
       "  'not',\n",
       "  'influence',\n",
       "  'abatacept',\n",
       "  'clearance',\n",
       "  '.'],\n",
       " 'false',\n",
       " 'none')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XMLdata[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of features\n",
    "Before training our model, we need to come up with features to help us determine whether there is a relationship between the two drugs or not.\n",
    "\n",
    "Some ideas for features are the following:\n",
    "- Does the sentence contain a modal verb (should, must,...) between the two entities?\n",
    "- Word bigrams: This is a binary feature for all word bigrams that appeared more than once in the corpus, indicating the presence or absence of each such bigram in the sentence\n",
    "- Number of words between a pair of drugs\n",
    "- Number of drugs between a pair of drugs\n",
    "- POS of words between a pair of drugs: This is a binary feature for word POS tags obtained from POS tagging, and indicates the presence or absence of each POS between the two main drugs.\n",
    "- Path between a pair of drugs: Path between two main drugs in the parse tree is another feature in our system. Because syntactic paths are in general a sparse feature, we reduced the sparsity by collapsing identical adjacent non-terminal labels. E.g., NP-S-VP-VP-NP is converted to NP-S-VP-NP. This technique decreased the number of paths by 24.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with(open('data/DrugBank_names_DB.txt', 'r')) as f:\n",
    "    drugbank_db = f.read().splitlines()\n",
    "    \n",
    "def sent2features(tupl, i, database):\n",
    "    \n",
    "    if len(tupl) != 8:\n",
    "        raise ValueError('The introduced tuple does not have the correct length')\n",
    "    sent = tupl[0]\n",
    "    ent1 = tupl[1]\n",
    "    ent2 = tupl[2]\n",
    "    ent_list = tupl[3]\n",
    "    pos_tags = tupl[4]\n",
    "    tok_sent = tupl[5] # Tokenized sentence\n",
    "    \n",
    "    prefixes = r'^alk|^meth|^eth|^prop|^but|^pent|^hex|^hept|^oct|^non|^dec|^undec|^dodec|^eifcos|^di|^tri|^tetra|^penta|^hexa|^hepta'\n",
    "    suffixes = r'ane$|ene$|yne$|yl$|ol$|al$|oic$|one$|ate$|amine$|amide$'\n",
    "\n",
    "    features = {\n",
    "        \n",
    "    'ent1': ent1,\n",
    "    'ent2': ent2,\n",
    "    # Orthographic features\n",
    "        \n",
    "    # Entity 1\n",
    "    'ent1_all_uppercase_letters' : allCaps(ent1), \n",
    "    'ent1_initial_capital_letter': initCap(ent1), \n",
    "    'ent1_contains_capital_letter' : hasCap(ent1),\n",
    "    'ent1_single_capital_letter' : singleCap(ent1),\n",
    "    'ent1_punctuation' : punctuation(ent1),\n",
    "    'ent1_initial_digit' : initDigit(ent1),\n",
    "    'ent1_single_digit' : singleDigit(ent1),\n",
    "    'ent1_letter_and_num' : alphaNum(ent1),\n",
    "    'ent1_many_numbers' : manyNum(ent1),\n",
    "    'ent1_contains_real_numbers' : realNum(ent1),\n",
    "    'ent1_intermediate_dash' : inDash(ent1),\n",
    "    'ent1_has_digit' : hasDigit(ent1),\n",
    "    'ent1_is_Dash' : isDash(ent1),\n",
    "    'ent1_is_roman_letter' : roman(ent1),\n",
    "    'ent1_is_end_punctuation' : endPunctuation(ent1),\n",
    "    'ent1_caps_mix' : capsMix(ent1),\n",
    "\n",
    "    # Entity 2\n",
    "    'ent2_all_uppercase_letters' : allCaps(ent2), \n",
    "    'ent2_initial_capital_letter': initCap(ent2), \n",
    "    'ent2_contains_capital_letter' : hasCap(ent2),\n",
    "    'ent2_single_capital_letter' : singleCap(ent2),\n",
    "    'ent2_punctuation' : punctuation(ent2),\n",
    "    'ent2_initial_digit' : initDigit(ent2),\n",
    "    'ent2_single_digit' : singleDigit(ent2),\n",
    "    'ent2_letter_and_num' : alphaNum(ent2),\n",
    "    'ent2_many_numbers' : manyNum(ent2),\n",
    "    'ent2_contains_real_numbers' : realNum(ent2),\n",
    "    'ent2_intermediate_dash' : inDash(ent2),\n",
    "    'ent2_has_digit' : hasDigit(ent2),\n",
    "    'ent2_is_Dash' : isDash(ent2),\n",
    "    'ent2_is_roman_letter' : roman(ent2),\n",
    "    'ent2_is_end_punctuation' : endPunctuation(ent2),\n",
    "    'ent2_caps_mix' : capsMix(ent2),\n",
    "        \n",
    "    # Morphological information: prefixes/suffixes of lengths from 2 to 5 and word shapes of tokens. \n",
    "    # Entity 1\n",
    "    'ent1_word[-5:]': ent1[-5:],\n",
    "    'ent1_word[-4:]': ent1[-4:],\n",
    "    'ent1_word[-3:]': ent1[-3:],\n",
    "    'ent1_word[-2:]': ent1[-2:],\n",
    "\n",
    "    # Entity 2\n",
    "    'ent2_word[-5:]': ent2[-5:],\n",
    "    'ent2_word[-4:]': ent2[-4:],\n",
    "    'ent2_word[-3:]': ent2[-3:],\n",
    "    'ent2_word[-2:]': ent2[-2:],\n",
    "    \n",
    "    # Domain knowledge\n",
    "    # Entity 1\n",
    "    'ent1_drug_sufix': getSuffix(ent1, suffixes),\n",
    "    'ent1_drug_prefix': getPrefix(ent1, prefixes),\n",
    "\n",
    "    # Entity 2\n",
    "    'ent2_drug_sufix': getSuffix(ent2, suffixes),\n",
    "    'ent2_drug_prefix': getPrefix(ent2, prefixes),\n",
    "\n",
    "        \n",
    "    # Is in DrugBank dataset\n",
    "    'ent1_isInDB':isTokenInDB(ent1,database),\n",
    "    'ent2_isInDB':isTokenInDB(ent2,database),\n",
    "    \n",
    "        \n",
    "    # Context features\n",
    "    'n_tokens_bw_entities': countTokensBetweenEntities(tok_sent, ent1, ent2),\n",
    "    'n_entities_bw_entities': countEntitiesBetweenEntities(tok_sent, ent1, ent2, ent_list),\n",
    "    'n_modal_verbs_bw_entities': countModalVerbsBetweenEntities(tok_sent, ent1, ent2),\n",
    "    'sentence_contains_neg': sentenceContainsNegation(tok_sent),\n",
    "    'keywords_bw_entities': keyWordsBetweenEntities(tok_sent, ent1, ent2),\n",
    "    'first_modal_sentence': getFirstModalVerb(tok_sent),\n",
    "    'POS_tags_sentence_simpl': createSimplifiedPOSPath(pos_tags),\n",
    "    '2_grams_bw_entities': getNgramsBetweenEntities(tok_sent, ent1, ent2, 2),\n",
    "    '3_grams_bw_entities': getNgramsBetweenEntities(tok_sent, ent1, ent2, 3)\n",
    "        \n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def text2features(text,database):\n",
    "    for i in range(len(text)):\n",
    "        return(sent2features(text, i, drugbank_db))\n",
    "\n",
    "def text2labels(text):\n",
    "    return text[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 4.75 ms, total: 147 ms\n",
      "Wall time: 145 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = [[text2features(s, drugbank_db)] for s in XMLdata]\n",
    "y = [[text2labels(s)] for s in XMLdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Validation split\n",
    "\n",
    "For this project, we will split the original dataset in the following proportions:\n",
    " - Training data: 42%\n",
    " - Testing data: 40%\n",
    " - Validation data: 18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences:  128\n",
      "Number of testing sentences:  124\n"
     ]
    }
   ],
   "source": [
    "seed = 16273\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed, shuffle = True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=seed, shuffle = True)\n",
    "print('Number of training sentences: ', len(X_train))\n",
    "print('Number of testing sentences: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'JJ', 'NNS', 'VBD', 'CD', 'CC', 'JJR', 'IN', 'DT', 'JJ', 'JJ', 'NNS', 'IN', 'NNP', ':', 'NNP', ',', 'NNP', ',', 'NNS', ',', 'NNP', 'VBG', 'NNS', ',', 'NN', ',', 'NN', ',', 'NN', ',', 'NN', ',', 'RB', ',', 'NN', ',', 'CC', 'NN', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization\n",
    "\n",
    "\n",
    "Using validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected bytes, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-beaed5064f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# We don't want to include the 'none' label for the optimization\\nlabels = ['mechanism', 'effect', 'int', 'advise']\\n\\n# Define fixed parameters and parameters to search\\ncrf = sklearn_crfsuite.CRF(\\n    algorithm='lbfgs',\\n    max_iterations=100,\\n    all_possible_transitions=True\\n)\\n\\n\\n# search\\n'''\\n\\n# use the same metric for evaluation\\nf1_scorer = make_scorer(metrics.flat_f1_score,\\n                        average='weighted', labels=labels)\\n\\n\\nparams_space = {\\n    'c1': scipy.stats.expon(scale=0.5),\\n    'c2': scipy.stats.expon(scale=0.05),\\n}\\n\\nrs = RandomizedSearchCV(crf, params_space,\\n                        cv=3,\\n                        verbose=1,\\n                        n_jobs=-1,\\n                        n_iter=20,\\n                        scoring=f1_scorer)\\n\\n\\n'''\\n\\ncrf.fit(X_val, y_val)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_dev, y_dev)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.append\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/pycrfsuite/_pycrfsuite.cpython-35m-darwin.so\u001b[0m in \u001b[0;36mvector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/cesc/Anaconda3/anaconda/lib/python3.5/site-packages/pycrfsuite/_pycrfsuite.cpython-35m-darwin.so\u001b[0m in \u001b[0;36mstring.from_py.__pyx_convert_string_from_py_std__in_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, list found"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We don't want to include the 'none' label for the optimization\n",
    "labels = ['mechanism', 'effect', 'int', 'advise']\n",
    "\n",
    "# Define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "\n",
    "# search\n",
    "'''\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=20,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "crf.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Using training data and the parameters obtained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=rs.best_params_['c1'],\n",
    "    c2=rs.best_params_['c2'],\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] is None:\n",
    "        print('y_pred index: ', i)\n",
    "        y_pred[i][0] = 'none'\n",
    "    \n",
    "    if y_test[i][0] is None:\n",
    "        print('y_test index:' , i)\n",
    "        y_test[i][0] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.recall_score(y_true = y_test, \n",
    "                             y_pred = y_pred, \n",
    "                             labels=labels, \n",
    "                             pos_label=1, \n",
    "                             average='weighted',\n",
    "                             sample_weight=None)\n",
    "      )\n",
    "\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(collections.Counter(crf.state_features_).most_common(15))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(collections.Counter(crf.state_features_).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def transformStrCategoriesIntoInts(vector):\n",
    "    res = []\n",
    "    for el in vector:\n",
    "        if el == 'none' or el == 'None':\n",
    "            res.append(0)\n",
    "        elif el == 'mechanism':\n",
    "            res.append(1)\n",
    "        elif el == 'effect':\n",
    "            res.append(2)\n",
    "        elif el == 'int':\n",
    "            res.append(3)\n",
    "        elif el == 'advise':\n",
    "            res.append(4)\n",
    "        else:\n",
    "            print(el)\n",
    "            print(type(el))\n",
    "            print(vector.index(el))\n",
    "    return(res)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
