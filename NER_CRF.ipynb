{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Name Entity Classifier\n",
    "## AHLT - MIRI 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Load needed modules and specify the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,KFold # Parameter selection\n",
    "import time # Execution time of some blocks\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import statistics\n",
    "import scipy.stats # for RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Import our defined functions\n",
    "from drug_functions import *\n",
    "from makingPredictions import *\n",
    "from datasetBuilder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init time\n",
    "first_init = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directories\n",
    "# train_dirs_whereto_parse = ['data/medium_train_DrugBank']\n",
    "# test_dirs_whereto_parse = ['data/medium_test_DrugBank']\n",
    "train_dirs_whereto_parse = ['data/Train/DrugBank','data/Train/MedLine']\n",
    "test_dirs_whereto_parse = ['data/Test/DrugBankOutput','data/Test/MedlineOutput']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train and test data from the XML files\n",
    "Accessing to all the files of the directory and storing id's and text's in two arrays.\n",
    "We have also added the token 'STOP' at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_entities = createTrainSet(train_dirs_whereto_parse)\n",
    "test_texts_entities = createTestSet(test_dirs_whereto_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing features, I want the input data to have a special format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences:  5560\n",
      "Num of test sentences:  665\n"
     ]
    }
   ],
   "source": [
    "# we want each dataset with the following format: \n",
    "# for ['have','Ibuprofeno'] ---- [('hola','V','O'),('Ibuprofeno','NN','B')]\n",
    "\n",
    "def buildSet(text_entities):\n",
    "    dataset = []\n",
    "    for text,drugs in text_entities:\n",
    "        # tokenizing\n",
    "        tokenized_sentence = nltk.word_tokenize(text)\n",
    "        # BIO tagging\n",
    "        tokens_tags = BIOTagger(text, drugs)\n",
    "        # POS tagging\n",
    "        tokens_pos = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "        text_triples = []\n",
    "        for idx,token in enumerate(tokenized_sentence):\n",
    "            text_triples.append((token,tokens_pos[idx][1],tokens_tags[idx][1]))\n",
    "        dataset.append(text_triples)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# train_set and test_set are list of list of triples; each list of triples refers to a different sentence\n",
    "train_set = buildSet(train_texts_entities) \n",
    "test_set = buildSet(test_texts_entities)\n",
    "\n",
    "print('Number of training sentences: ',len(train_set))\n",
    "print('Num of test sentences: ',len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # remember s is a triple defined as follows: (token, POS tag, BIO tag)\n",
    "X_train = [sent2features(s) for s in train_set]\n",
    "y_train = [sent2labels(s) for s in train_set]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_set]\n",
    "y_test = [sent2labels(s) for s in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags = crf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.07\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "recall = []\n",
    "for idx, text_drugs in enumerate(test_texts_entities): \n",
    "    # so text_drugs will be a tuple of (sentence, list of true entities in that sentence)\n",
    "    tokens = nltk.word_tokenize(text_drugs[0])\n",
    "    predicted_entities = BIOTagsToEntities(tokens,pred_tags[idx])\n",
    "    precision = precision + [compute_precision(predicted_entities,text_drugs[1])]\n",
    "    recall = recall + [compute_recall(predicted_entities,text_drugs[1])]\n",
    "\n",
    "avg_precision = statistics.mean(precision)\n",
    "avg_recall = statistics.mean(recall)\n",
    "\n",
    "# F1 metric\n",
    "F1 = round((2*avg_precision*avg_recall) / (avg_precision + avg_recall),2)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time:  24.08883309364319  seconds\n"
     ]
    }
   ],
   "source": [
    "print('Total execution time: ',(time.time() - first_init), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Log of results\\ndate, precision, recall, F1, features, test\\n14-May, 46.2, 52.1, 48.99, Token length; Prefixes/Suffixes; POS tag; Binary features (+-2); Token position; DrugBank DB; Shape, yes\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "## Log of results\n",
    "date, precision, recall, F1, features, test\n",
    "14-May, 46.2, 52.1, 48.99, Token length; Prefixes/Suffixes; POS tag; Binary features (+-2); Token position; DrugBank DB; Shape, yes\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
